# Iceberg2 template - DuckDB-based Iceberg implementation
# Uses DuckDB's native Iceberg extension with information_schema for metadata

core:
  drop_table: drop table if exists {table}
  drop_view: drop view if exists {view}
  drop_index: drop index if exists {index}
  create_index: create index {index} on {table} ({cols})
  create_unique_index: create unique index {index} on {table} ({cols})
  create_table: create table if not exists {table} ({col_types})
  create_temporary_table: create temp table if not exists {table} ({col_types})
  replace: replace into {table} ({names}) values({values})
  truncate_table: delete from {table}
  insert_option: ""
  modify_column: 'alter {column} type {type}'
  select_stream_scanner: select {fields} from {stream_scanner} {where}

  # Export functionality (inherited from DuckDB)
  export_to_local: |
    COPY (
      select *
      from {table}
    ) TO '{local_path}'
    (
      format '{format}', overwrite true, {file_size_bytes_expr} {file_extension_expr}
      compression '{compression}'
    )
  export_to_local_partitions: |
    COPY (
      select
        *,
        {partition_expressions}
      from {table}
    ) TO '{local_path}'
    (
      format '{format}', {file_size_bytes_expr} {file_extension_expr}
      compression '{compression}',
      overwrite true,
      write_partition_columns {write_partition_columns},
      partition_by ( {partition_columns} )
    )

  # Cloud storage secrets
  default_s3_secret: |
    create or replace secret s3_default (
        type s3,
        provider credential_chain,
        chain 'env;config'
    )
  default_azure_secret: |
    create or replace secret azure_default (
        type azure,
        provider credential_chain,
        chain 'default;env;managed_identity;cli',
        account_name '{account}'
    )

metadata:
  databases: PRAGMA database_list

  current_database: PRAGMA database_list

  schemas: |
    select distinct schema_name
    from information_schema.schemata
    where catalog_name = current_catalog()
    order by schema_name

  tables: |
    select
      table_schema as schema_name,
      table_name,
      case table_type
        when 'VIEW' then 'true'
        else 'false'
      end as is_view
    from information_schema.tables
    where table_type != 'VIEW'
      {{if .schema -}} and table_schema = '{schema}' {{- end}}
    order by table_schema, table_name

  views: |
    select
      table_schema as schema_name,
      table_name,
      'true' as is_view
    from information_schema.tables
    where table_type = 'VIEW'
      {{if .schema -}} and table_schema = '{schema}' {{- end}}
    order by table_schema, table_name

  columns: |
    select
      column_name,
      data_type,
      coalesce(numeric_precision, character_maximum_length) as precision,
      numeric_scale as scale
    from information_schema.columns
    where table_schema = '{schema}'
      and table_name = '{table}'
    order by ordinal_position

  columns_temp: |
    select
      column_name,
      data_type,
      coalesce(numeric_precision, character_maximum_length) as precision,
      numeric_scale as scale
    from information_schema.columns
    where table_schema = '{schema}'
      and table_name = '{table}'
    order by ordinal_position

  primary_keys: |
    select '{table}.key' as pk_name,
           constraint_index as position,
           replace(replace(constraint_text, 'PRIMARY KEY(', ''), ')', '') as column_name
    from duckdb_constraints()
    where table_schema = '{schema}'
      and table_name = '{table}'
      and constraint_type = 'PRIMARY KEY'

  indexes: |
    select index_name as index_name,
           sql as column_name
    from duckdb_indexes()
    where table_schema = '{schema}'
      and table_name = '{table}'

  columns_full: |
    select
      cols.table_schema as schema_name,
      cols.table_name as table_name,
      cols.column_name as column_name,
      cols.data_type as data_type,
      cols.ordinal_position as position
    from information_schema.columns cols
    where table_catalog = current_catalog()
      and table_schema = '{schema}'
      and table_name = '{table}'
    order by cols.table_schema, cols.table_name, cols.ordinal_position

  schemata: |
    select
      cols.table_schema as schema_name,
      cols.table_name as table_name,
      case tables.table_type
        when 'VIEW' then true
        else false
      end as is_view,
      cols.column_name,
      cols.data_type,
      cols.ordinal_position as position
    from information_schema.columns cols
    join information_schema.tables tables
      on tables.table_schema = cols.table_schema
      and tables.table_name = cols.table_name
    where cols.table_catalog = current_catalog()
      {{if .schema -}} and cols.table_schema = '{schema}' {{- end}}
      {{if .tables -}} and cols.table_name in ({tables}) {{- end}}
    order by schema_name, table_name, position

  ddl_table: |
    PRAGMA table_info('{schema}.{table}')

  ddl_view: |
    PRAGMA table_info('{schema}.{table}')

analysis:
  chars: |
    select
      '{schema}' as schema_nm,
      '{table}' as table_nm,
      '{field}' as field, sum(case when {field}::text ~ '\n' then 1 else 0 end) as cnt_nline,
      sum(case when {field}::text ~ '\t' then 1 else 0 end) as cnt_tab,
      sum(case when {field}::text ~ ',' then 1 else 0 end) as cnt_comma,
      sum(case when {field}::text ~ '"' then 1 else 0 end) as cnt_dquote,
      min(length({field}::text)) as f_min_len,
      max(length({field}::text)) as f_max_len
    from "{schema}"."{table}"

function:
  sleep: select sqlite3_sleep({seconds}*1000)
  checksum_datetime: CAST((epoch({field}) || substr(strftime({field}, '%f'),4) ) as bigint)
  checksum_decimal: 'abs(cast({field} as bigint))'
  checksum_boolean: 'length({field}::string)'
  cast_to_text: 'cast({field} as text)'

  # Scanner functions
  iceberg_scanner: iceberg_scan('{uri}', allow_moved_paths = true)
  delta_scanner: delta_scan('{uri}')
  parquet_scanner: read_parquet([{uris}]{filename_expr})
  csv_scanner: read_csv([{uris}], delim='{delimiter}', header={header}, max_line_size=2000000, parallel=true, quote='{quote}', escape='{escape}', nullstr='{null_if}'{filename_expr})

variable:
  bool_as: integer
  bind_string: ${c}
  batch_rows: 50
  batch_values: 1000
  timestamp_layout: '2006-01-02 15:04:05.000000'
  timestampz_layout: '2006-01-02 15:04:05.000000-07:00'
  max_string_type: text
  max_string_length: 2147483647
  quote_char: '"'
