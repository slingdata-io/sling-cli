core:
  drop_table: drop table if exists {table}
  drop_view: drop view if exists {view}
  drop_index: drop index if exists {index}
  create_index: create index {index} on {table} ({cols})
  create_unique_index: create unique index {index} on {table} ({cols})
  create_table: create table if not exists {table} ({col_types})
  create_temporary_table: create temp table if not exists {table} ({col_types})
  replace: replace into {table} ({names}) values({values})
  truncate_table: delete from {table}
  insert_option: ""
  modify_column: 'alter {column} type {type}'
  select_stream_scanner: select {fields} from {stream_scanner} {where}
  export_to_local: |
    COPY (
      select *
      from {table}
    ) TO '{local_path}' 
    (
      format '{format}', overwrite true, {file_size_bytes_expr} {file_extension_expr}
      compression '{compression}'
    )
  export_to_local_partitions: |
    COPY (
      select
        *,
        {partition_expressions}
      from {table}
    ) TO '{local_path}'
    (
      format '{format}', {file_size_bytes_expr} {file_extension_expr}
      compression '{compression}',
      overwrite true,
      write_partition_columns {write_partition_columns},
      partition_by ( {partition_columns} )
    )

  # DuckDB supports 3 merge strategies (update_insert requires PK constraint)
  merge_insert: |
    INSERT INTO {tgt_table} ({insert_fields})
    SELECT {src_fields} FROM {src_table} src

  merge_update: |
    UPDATE {tgt_table} tgt
    SET {set_fields}
    FROM {src_table} src
    WHERE {src_tgt_pk_equal}

  # DuckDB's INSERT OR REPLACE requires a PRIMARY KEY constraint on the table
  # Since sling creates tables without explicit PK constraints, this strategy is disabled
  merge_update_insert: null

  merge_delete_insert: |
    DELETE FROM {tgt_table} tgt
    WHERE EXISTS (
      SELECT 1 FROM {src_table} src
      WHERE {src_tgt_pk_equal}
    );
    INSERT INTO {tgt_table} ({insert_fields})
    SELECT {src_fields} FROM {src_table} src

metadata:
  databases: PRAGMA database_list
  
  current_database: PRAGMA database_list

  schemas: |
    select distinct schema_name
    from information_schema.schemata
    order by schema_name

  tables: |
    select table_schema as schema_name, table_name, 'false' as is_view
    from information_schema.tables
    where table_type = 'BASE TABLE'
      {{if .schema -}} and table_schema = '{schema}' {{- end}}
    order by table_schema, table_name


  views: |
    select table_schema as schema_name, table_name, 'true' as is_view
    from information_schema.tables
    where table_type in ('VIEW')
      {{if .schema -}} and table_schema = '{schema}' {{- end}}
    order by table_schema, table_name

  columns: |
    select column_name, data_type, coalesce(numeric_precision, character_maximum_length) as precision, numeric_scale as scale
    from information_schema.columns
    where table_schema = '{schema}'
      and table_name = '{table}'
    order by ordinal_position

  primary_keys: |
    select '{table}.key' as pk_name,
           constraint_index as position,
           replace(replace(constraint_text, 'PRIMARY KEY(', ''), ')', '') as column_name
    from duckdb_constraints()
    where table_schema = '{schema}'
      and table_name = '{table}'
      and constraint_type = 'PRIMARY KEY'
  
  indexes: |
    select index_name as index_name,
           sql as column_name
    from duckdb_indexes()
    where table_schema = '{schema}'
      and table_name = '{table}'

  columns_full: |
    with tables_cte as (
      select
        table_catalog,
        table_schema,
        table_name,
        case table_type
          when 'VIEW' then true
          when 'FOREIGN' then true
          else false
        end as is_view
      from information_schema.tables
      where table_schema = '{schema}'
        and table_name = '{table}'
    )
    select
      cols.table_schema as schema_name,
      cols.table_name as table_name,
      cols.column_name as column_name,
      cols.data_type as data_type,
      cols.ordinal_position as position
    from information_schema.columns cols
    join tables_cte
      on tables_cte.table_schema = cols.table_schema
      and tables_cte.table_name = cols.table_name
    order by cols.table_catalog, cols.table_schema, cols.table_name, cols.ordinal_position
  
  schemata: |
    with tables_cte as (
      select
        table_catalog,
        table_schema,
        table_name,
        case table_type
          when 'VIEW' then true
          else false
        end as is_view
      from information_schema.tables
      where 1=1
        {{if .schema -}} and table_schema = '{schema}' {{- end}}
        {{if .tables -}} and table_name in ({tables}) {{- end}}
    )
    select
      cols.table_schema as schema_name,
      cols.table_name as table_name,
      tables_cte.is_view as is_view,
      cols.column_name as column_name,
      cols.data_type as data_type,
      cols.ordinal_position as position
    from information_schema.columns cols
    join tables_cte
      on tables_cte.table_schema = cols.table_schema
      and tables_cte.table_name = cols.table_name
    order by cols.table_catalog, cols.table_schema, cols.table_name, cols.ordinal_position
  
  ddl_table: |
    PRAGMA table_info('{schema}.{table}')
  
  ddl_view: |
    PRAGMA table_info('{schema}.{table}')

analysis:
  chars: |
    select
      '{schema}' as schema_nm,
      '{table}' as table_nm,
      '{field}' as field, sum(case when {field}::text ~ '\n' then 1 else 0 end) as cnt_nline, 
      sum(case when {field}::text ~ '\t' then 1 else 0 end) as cnt_tab, 
      sum(case when {field}::text ~ ',' then 1 else 0 end) as cnt_comma, 
      sum(case when {field}::text ~ '"' then 1 else 0 end) as cnt_dquote, 
      min(length({field}::text)) as f_min_len, 
      max(length({field}::text)) as f_max_len
    from "{schema}"."{table}"

  fields: |
  fields_deep: |
  fields_distro: |
  fields_distro_group: |
  fields_date_distro: |
  fields_date_distro_wide: |
  fields_group: |

function:
  sleep: select sqlite3_sleep({seconds}*1000)
  checksum_datetime: CAST((epoch({field}) || substr(strftime({field}, '%f'),4) ) as bigint)
  checksum_decimal: 'abs(cast({field} as bigint))'
  checksum_boolean: 'length({field}::string)'
  cast_to_text: 'cast({field} as text)'

  iceberg_scanner: iceberg_scan('{uri}', allow_moved_paths = true)
  delta_scanner: delta_scan('{uri}')
  parquet_scanner: read_parquet([{uris}]{filename_expr})
  # csv_scanner: read_csv('{uri}', delim='{delimiter}', header={header}, columns={columns}, max_line_size=2000000, parallel=true, quote='{quote}', escape='{escape}', nullstr='{null_if}')
  csv_scanner: read_csv([{uris}], delim='{delimiter}', header={header}, max_line_size=2000000, parallel=true, quote='{quote}', escape='{escape}', nullstr='{null_if}'{filename_expr})

variable:
  # DuckDB default is delete_insert as it's more reliable
  default_merge_strategy: delete_insert
  bool_as: integer
  bind_string: ${c}
  batch_rows: 50
  batch_values: 1000
  timestamp_layout: '2006-01-02 15:04:05.000000'
  timestampz_layout: '2006-01-02 15:04:05.000000-07:00'
  max_string_type: text
  max_string_length: 2147483647

native_type_map:
  bigint: bigint
  binary: binary
  blob: binary
  boolean: bool
  char: string
  date: date
  datetime: datetime
  decimal: decimal
  double: float
  enum: string
  float: float
  hugeint: bigint
  integer: integer
  interval: string
  json: json
  list: text
  map: json
  smallint: smallint
  struct: json
  text: text
  time: time
  timestamp: timestamp
  "timestamp with time zone": timestampz
  tinyblob: text
  tinyint: smallint
  ubigint: bigint
  uinteger: bigint
  usmallint: integer
  utinyint: integer
  uuid: uuid
  varchar: text

general_type_map:
  bigint: bigint
  binary: binary
  bool: bool
  date: date
  datetime: datetime
  decimal: "decimal(,)"
  float: double
  integer: integer
  json: json
  smallint: smallint
  string: "varchar()"
  text: text
  time: time
  timestamp: timestamp
  timestampz: timestamptz
  timez: time
  uuid: uuid
