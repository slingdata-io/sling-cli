core:
  drop_table: drop table if exists {table}
  drop_view: drop view if exists {view}
  drop_index: drop index if exists {index}
  create_index: create index if not exists {index} on {table} ({cols})
  create_schema: create schema if not exists {schema}
  drop_schema: drop schema if exists {schema} cascade
  create_table: create table if not exists {table} ({col_types})
  rename_table: alter table {table} rename to {new_table}
  rename_column: alter table {table} rename column {column} to {new_column}
  alter_columns: alter table {table} modify column {col_ddl}
  modify_column: '{column} {type}'
  truncate_table: truncate table {table}
  insert: insert into {table} ({fields}) values ({values})
  update: update {table} set {set_fields} where {pk_fields_equal}
  delete: delete from {table} where {where}
  replace: |
    merge into {table} tgt
    using (select {name_values}) src
    on ({src_tgt_condition})
    when matched then
      update set {set_fields}
    when not matched then
      insert ({names}) values ({values})
  upsert: |
    merge into {tgt_table} tgt
    using {src_table} src
    on ({src_tgt_pk_equal})
    when matched then
      update set {set_fields}
    when not matched then
      insert ({insert_fields}) values ({placeholder_fields})
  delete_where_not_exist: |
    delete from {target_table}
    where {where}
      and not exists (
          select 1 from {temp_table}
          where {join_where}
      )
  update_where_not_exist: |
    update {target_table}
    set {set_fields}
    where {where}
      and not exists (
          select 1 from {temp_table} 
          where {join_where}
      )
  # offset with custom sql requires order by
  limit_sql: |
    select * from (
      {sql}
    ) as t limit {limit} /* offset {offset} */

  # Exasol supports all 4 merge strategies
  merge_insert: |
    INSERT INTO {tgt_table} ({insert_fields})
    SELECT {src_fields} FROM {src_table} src

  merge_update: |
    UPDATE {tgt_table} tgt
    SET {set_fields}
    FROM {src_table} src
    WHERE {src_tgt_pk_equal}

  merge_update_insert: |
    MERGE INTO {tgt_table} tgt
    USING {src_table} src
    ON ({src_tgt_pk_equal})
    WHEN MATCHED THEN UPDATE SET {set_fields}
    WHEN NOT MATCHED THEN INSERT ({insert_fields}) VALUES ({placeholder_fields})

  merge_delete_insert: |
    DELETE FROM {tgt_table} tgt
    WHERE EXISTS (
      SELECT 1 FROM {src_table} src
      WHERE {src_tgt_pk_equal}
    );
    INSERT INTO {tgt_table} ({insert_fields})
    SELECT {src_fields} FROM {src_table} src

metadata:
  
  current_database: |
    select current_database
  
  databases: |
    select schema_name as name
    from EXA_ALL_SCHEMAS
    order by schema_name
  
  schemas: |
    select schema_name
    from EXA_ALL_SCHEMAS
    where 1=1
      {{if .schema -}} and schema_name = '{schema}' {{- end}}
    order by schema_name
  
  schemata: |
    select 
      c.column_schema as schema_name,
      c.column_table as table_name,
      'table' as table_type,
      c.column_name,
      c.column_type as data_type,
      c.column_is_nullable as is_nullable,
      c.column_ordinal_position as "position"
    from EXA_ALL_COLUMNS c
    where 1=1
      {{if .schema -}} and c.column_schema = '{schema}' {{- end}}
      {{if .tables -}} and c.column_table in ({tables}) {{- end}}
    order by c.column_schema, c.column_table, c.column_ordinal_position
    
  tables: |
    select 
      table_schema as schema_name,
      table_name,
      'false' as is_view
    from EXA_ALL_TABLES
    where 1=1
      {{if .schema -}} and table_schema = '{schema}' {{- end}}
      {{if .table -}} and table_name = '{table}' {{- end}}
    order by table_schema, table_name
  
  views: |
    select 
      view_schema as schema_name,
      view_name as table_name,
      'true' as is_view
    from EXA_ALL_VIEWS
    where 1=1
      {{if .schema -}} and view_schema = '{schema}' {{- end}}
      {{if .view -}} and view_name = '{view}' {{- end}}
    order by view_schema, view_name
      
  columns: |
    select
      column_schema as schema_name,
      column_table as table_name,
      column_name,
      column_type as data_type,
      column_is_nullable as is_nullable,
      column_default as column_default,
      column_ordinal_position as ordinal_position
    from EXA_ALL_COLUMNS
    where 1=1
      {{if .schema -}} and column_schema = '{schema}' {{- end}}
      {{if .table -}} and column_table = '{table}' {{- end}}
    order by column_schema, column_table, column_ordinal_position
  
  primary_keys: |
    select
      constraint_schema as schema_name,
      constraint_table as table_name,
      column_name
    from EXA_ALL_CONSTRAINT_COLUMNS
    where constraint_type = 'PRIMARY KEY'
      {{if .schema -}} and constraint_schema = '{schema}' {{- end}}
      {{if .table -}} and constraint_table = '{table}' {{- end}}
    order by constraint_schema, constraint_table, ordinal_position
  
  indexes: |
    select
      table_schema as schema_name,
      table_name,
      column_name as index_name,
      column_name
    from EXA_ALL_COLUMNS
    where 1=1
      {{if .schema -}} and table_schema = '{schema}' {{- end}}
      {{if .table -}} and table_name = '{table}' {{- end}}
    order by table_schema, table_name, column_ordinal_position
  
  ddl: |
    select ddl
    from (
      select 
        'CREATE TABLE ' || table_schema || '.' || table_name || ' (' ||
        string_agg(
          column_name || ' ' || column_type || 
          case when column_is_nullable = 'NO' then ' NOT NULL' else '' end ||
          case when column_default is not null then ' DEFAULT ' || column_default else '' end,
          ', '
        ) || ')' as ddl
      from EXA_ALL_COLUMNS
      where table_schema = '{schema}'
        and table_name = '{table}'
      group by table_schema, table_name
    ) t

analysis:
  # placeholder

general:
  bind_string: "?"
  quote_char: '"'
  ddl_sep: ";"
  column_upper: true
  max_column_length: 128
  boolean_data_type: boolean
  
bulk:
  import: |
    IMPORT INTO {table} ({columns})
    FROM LOCAL CSV FILE '{file_path}'
    COLUMN SEPARATOR = ','
    COLUMN DELIMITER = '"'
    ROW SEPARATOR = 'LF'
    SKIP = 1
  
  export: |
    EXPORT ({sql})
    INTO LOCAL CSV FILE '{file_path}'
    COLUMN SEPARATOR = ','
    COLUMN DELIMITER = '"'
    ROW SEPARATOR = 'LF'
    WITH COLUMN NAMES

function:
  add_months: add_months({field}, {num})
  cast: cast({field} as {type})
  concat: concat({fields})
  date_diff_days: days_between({date1}, {date2})
  date_diff_seconds: seconds_between({date1}, {date2})
  date_parse_format: to_timestamp({string}, {format})
  date_to_epoch_ms: "(floor((cast({field} as timestamp) - timestamp '1970-01-01 00:00:00') * 86400) * 1000)"
  date_format: to_char({field}, {format})
  date_trunc_key: date_trunc('{unit}', {field})
  epoch_ms_to_date: "(timestamp '1970-01-01 00:00:00' + ({field} / 1000) * interval '1' second)"
  string: varchar(2000000)
  replace: replace({string}, {search}, {replacement})
  strcmp_format: "if({s1} = {s2} then 0 elif {s1} < {s2} then -1 else 1 end"
  datediff_format: "days_between({date1}, {date2})"
  hash: hashtype_md5({fields})
  random: random()
  uuid: sys_guid()
  
variable:
  column_upper: true
  bool_as: string
  duplicates_group_by: false
  handle_null_compare: true
  null_if_empty: false
  timestamp_layout: '2006-01-02 15:04:05.000000'
  timestampz_layout: '2006-01-02 15:04:05.000000'

native_type_map:
  bigint: bigint
  boolean: bool
  char: string
  date: date
  decimal: decimal
  double: float
  geometry: text
  hashtype: string
  integer: integer
  "interval day to second": string
  "interval year to month": string
  smallint: smallint
  timestamp: timestamp
  "timestamp with local time zone": timestampz
  varchar: string

general_type_map:
  bigint: bigint
  binary: "varchar(2000000)"
  bool: boolean
  date: date
  datetime: timestamp
  decimal: "decimal(,)"
  float: "double precision"
  integer: integer
  json: "varchar(2000000)"
  smallint: smallint
  string: "varchar(2000000)"
  text: "varchar(2000000)"
  time: "varchar(100)"
  timestamp: timestamp
  timestampz: "timestamp with local time zone"
  timez: "varchar(100)"
  uuid: "varchar(36)"
