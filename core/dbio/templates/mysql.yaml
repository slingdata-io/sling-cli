core:
  drop_table: drop table if exists {table}
  drop_view: drop view if exists {view}
  drop_index: "select 'cannot drop if exists index for mysql' as col1"
  create_table: create table if not exists {table} ({col_types})
  create_index: create index {index} on {table} ({cols})
  insert: insert into {table} ({fields}) values ({values})
  update: update {table} set {set_fields} where {pk_fields_equal}
  alter_columns: alter table {table} modify {col_ddl}
  modify_column: '{column} {type}'

  # MySQL only supports insert and delete_insert (no native MERGE support)
  merge_insert: |
    INSERT INTO {tgt_table} ({insert_fields})
    SELECT {src_fields} FROM {src_table} src

  merge_delete_insert: |
    DELETE FROM {tgt_table}
    WHERE EXISTS (
      SELECT 1 FROM {src_table}
      WHERE {src_tgt_pk_equal}
    );

    INSERT INTO {tgt_table} ({insert_fields})
    SELECT {src_fields} FROM {src_table}

  # MySQL does not support UPDATE or UPDATE_INSERT (no native MERGE statement)
  merge_update: null
  merge_update_insert: null

  load_data_local_reader: |
    LOAD DATA LOCAL INFILE 'Reader::{handler_name}'
    INTO TABLE {table}
    FIELDS TERMINATED BY ','
    OPTIONALLY ENCLOSED BY '"'
    ESCAPED BY '\\'
    LINES TERMINATED BY '\n'
    IGNORE 1 LINES

  # Foreign key DDL template
  add_foreign_key: |
    ALTER TABLE {table} ADD CONSTRAINT {constraint} FOREIGN KEY ({column}) REFERENCES {ref_table}({ref_column}){on_delete}{on_update}

  # Column comment/description template (MySQL requires column type when modifying)
  add_column_comment: ALTER TABLE {table} MODIFY COLUMN `{column}` {type} COMMENT {comment}

  # Table comment/description template
  add_table_comment: ALTER TABLE {table} COMMENT = {comment}

metadata:
  current_database: select database() as name from dual
  
  databases: select database() as name from dual
    
  schemas: |
    select schema_name
    from information_schema.schemata
    order by schema_name
    
  tables: |
    select table_schema as schema_name, table_name, 'false' as is_view
    from information_schema.tables
    where table_type = 'BASE TABLE'
      {{if .schema -}} and table_schema = '{schema}' {{- end}}
    order by table_schema, table_name
    
  views: |
    select table_schema as schema_name, table_name, 'true' as is_view
    from information_schema.tables
    where table_type = 'VIEW'
      {{if .schema -}} and table_schema = '{schema}' {{- end}}
    order by table_schema, table_name

  columns: |
    select column_name, data_type
    from information_schema.columns
    where table_schema = '{schema}'
      and table_name = '{table}'
    order by ordinal_position

  primary_keys: |
    select tco.constraint_name as pk_name,
           kcu.ordinal_position as position,
           kcu.column_name as column_name
    from information_schema.table_constraints tco
    join information_schema.key_column_usage kcu  
      on kcu.constraint_catalog = tco.constraint_catalog
      and kcu.constraint_schema = tco.constraint_schema
      and kcu.table_schema = tco.table_schema
      and kcu.table_name = tco.table_name
    where kcu.table_schema = '{schema}'
      and kcu.table_name = '{table}'
    order by kcu.table_schema,
             kcu.table_name,
             position

  indexes: |
    select
      index_name as index_name,
      column_name as column_name
    from information_schema.statistics
    where table_schema = '{schema}'
      and table_name = '{table}'
    order by
      index_name,
      seq_in_index

  # Extended column attributes for schema migration
  columns_extended: |
    SELECT
      c.column_name,
      CASE WHEN c.is_nullable = 'YES' THEN 'true' ELSE 'false' END AS is_nullable,
      c.column_default AS default_value,
      CASE WHEN c.extra LIKE '%auto_increment%' THEN 'true' ELSE 'false' END AS is_auto_increment,
      '1' AS identity_seed,
      '1' AS identity_increment,
      CASE WHEN pk.column_name IS NOT NULL THEN 'true' ELSE 'false' END AS is_primary_key,
      c.column_comment AS description
    FROM information_schema.columns c
    LEFT JOIN (
      SELECT kcu.table_schema, kcu.table_name, kcu.column_name
      FROM information_schema.table_constraints tc
      JOIN information_schema.key_column_usage kcu
        ON tc.constraint_name = kcu.constraint_name
        AND tc.table_schema = kcu.table_schema
        AND tc.table_name = kcu.table_name
      WHERE tc.constraint_type = 'PRIMARY KEY'
    ) pk ON pk.table_schema = c.table_schema
        AND pk.table_name = c.table_name
        AND pk.column_name = c.column_name
    WHERE c.table_schema = '{schema}' AND c.table_name = '{table}'
    ORDER BY c.ordinal_position

  # Foreign key relationships
  foreign_keys: |
    SELECT
      tc.constraint_name,
      kcu.column_name,
      kcu.referenced_table_schema AS referenced_schema,
      kcu.referenced_table_name AS referenced_table,
      kcu.referenced_column_name AS referenced_column,
      rc.delete_rule AS on_delete,
      rc.update_rule AS on_update
    FROM information_schema.table_constraints tc
    JOIN information_schema.key_column_usage kcu
      ON tc.constraint_name = kcu.constraint_name
      AND tc.table_schema = kcu.table_schema
    JOIN information_schema.referential_constraints rc
      ON rc.constraint_name = tc.constraint_name
      AND rc.constraint_schema = tc.constraint_schema
    WHERE tc.constraint_type = 'FOREIGN KEY'
      AND tc.table_schema = '{schema}'
      AND tc.table_name = '{table}'
    ORDER BY tc.constraint_name, kcu.ordinal_position

  # Extended table attributes for schema migration
  table_extended: |
    SELECT
      t.TABLE_COMMENT AS description
    FROM information_schema.tables t
    WHERE t.table_schema = '{schema}' AND t.table_name = '{table}'

  columns_full: |
    with tables as (
      select
        table_catalog,
        table_schema,
        table_name,
        case table_type
          when 'VIEW' then true
          else false
        end as is_view
      from information_schema.tables
      where table_schema = '{schema}' and table_name = '{table}'
    )
    select
      cols.table_schema as schema_name,
      cols.table_name as table_name,
      cols.column_name as column_name,
      cols.data_type as data_type,
      cols.ordinal_position as position
    from information_schema.columns cols
    join tables
      on tables.table_catalog = cols.table_catalog
      and tables.table_schema = cols.table_schema
      and tables.table_name = cols.table_name
    order by cols.table_catalog, cols.table_schema, cols.table_name, cols.ordinal_position

  schemata: |
    with tables as (
      select
        table_catalog,
        table_schema,
        table_name,
        case table_type
          when 'VIEW' then true
          else false
        end as is_view
      from information_schema.tables
      where 1=1
        {{if .schema -}} and table_schema = '{schema}' {{- end}}
        {{if .tables -}} and table_name in ({tables}) {{- end}}
    )
    select
      cols.table_schema as schema_name,
      cols.table_name as table_name,
      tables.is_view as is_view,
      cols.column_name as column_name,
      cols.data_type as data_type,
      cols.ordinal_position as position
    from information_schema.columns cols
    join tables
      on tables.table_catalog = cols.table_catalog
      and tables.table_schema = cols.table_schema
      and tables.table_name = cols.table_name
    order by cols.table_catalog, cols.table_schema, cols.table_name, cols.ordinal_position
  
  ddl_table: SHOW CREATE TABLE `{schema}`.`{table}`
  ddl_view: SHOW CREATE TABLE `{schema}`.`{table}`

analysis:
  # table level
  table_count: |
    -- table_count {table}
    select
      '{schema}' as schema_nm,
      '{table}' as table_nm,
      count(*) cnt
    from `{schema}`.`{table}`

  field_chars: |
    -- field_chars {table}
    select
      '{schema}' as schema_nm,
      '{table}' as table_nm,
      '{field}' as field,
      sum(case when regexp_like({field}, '\\n') then 1 else 0 end) as cnt_nline,
      sum(case when regexp_like({field}, '\\t') then 1 else 0 end) as cnt_tab,
      sum(case when regexp_like({field}, ',') then 1 else 0 end) as cnt_comma,
      sum(case when regexp_like({field}, '"') then 1 else 0 end) as cnt_dquote
    from `{schema}`.`{table}`

  field_pk_test: |
    -- field_pk_test {table}
    select
      '`{schema}`.`{table}`' as table_nm,
      case when count(*) = count(distinct {field}) then 'PASS' else 'FAIL' end as result,
      count(*) as tot_cnt,
      count(distinct {field}) as dstct_cnt
    from `{schema}`.`{table}`

  field_stat: |
    -- field_stat {field}
    select
      '{schema}' as schema_nm,
      '{table}' as table_nm,
      '{field}' as field,
      count(*) as tot_cnt,
      count({field}) as f_cnt,
      count(*) - count({field}) as f_null_cnt,
      round(100.0 * (count(*) - count({field})) / count(*), 1) as f_null_prct,
      count(distinct {field}) as f_dstct_cnt,
      round(100.0 * count(distinct {field}) / count(*), 1) as f_dstct_prct,
      count(*) - count(distinct {field}) as f_dup_cnt
    from `{schema}`.`{table}`

  field_stat_group: |
    -- field_stat_group {field}
    select
      '{schema}' as schema_nm,
      '{table}' as table_nm,
      {group_expr} as group_exp,
      '{field}' as field,
      count(*) as tot_cnt,
      count({field}) as f_cnt,
      count(*) - count({field}) as f_null_cnt,
      round(100.0 * (count(*) - count({field})) / count(*), 1) as f_null_prct,
      count(distinct {field}) as f_dstct_cnt,
      round(100.0 * count(distinct {field}) / count(*), 1) as f_dstct_prct,
      count(*) - count(distinct {field}) as f_dup_cnt
    from `{schema}`.`{table}`
    group by {group_expr}

  field_stat_deep: |
    -- field_stat_deep {field}
    select
      '{schema}' as schema_nm,
      '{table}' as table_nm,
      '{field}' as field,
      '{type}' as type,
      count(*) as tot_cnt,
      count({field}) as f_cnt,
      count(*) - count({field}) as f_null_cnt,
      round(100.0 * (count(*) - count({field})) / count(*), 1) as f_null_prct,
      count(distinct {field}) as f_dstct_cnt,
      round(100.0 * count(distinct {field}) / count(*), 1) as f_dstct_prct,
      count(*) - count(distinct {field}) as f_dup_cnt,
      cast(min({field}) as char(4000)) as f_min,
      cast(max({field}) as char(4000)) as f_max,
      min(length({field})) as f_min_len,
      max(length({field})) as f_max_len
    from `{schema}`.`{table}`

  fill_cnt_group_field: |
    -- fill_cnt_group_field {field}
    select
      {field},
      {columns_sql}
    from `{schema}`.`{table}`
    group by {field}
    order by {field}

  fill_rate_group_field: |
    -- fill_rate_group_field {field}
    select
      {field},
      {fill_rate_fields_sql}
    from `{schema}`.`{table}`
    group by {field}
    order by {field}

  distro_field: |
    -- distro_field {field}
    with t1 as (
      select
        '{field}' as field,
        {field},
        count(*) cnt
      from `{schema}`.`{table}`
      group by {field}
      order by count(*) desc
    )
    , t2 as (
      select
        '{field}' as field,
        count(*) ttl_cnt
      from `{schema}`.`{table}`
    )
    select
      '{table}' as table_nm,
      t1.field,
      {field} as value,
      cnt,
      round(100.0 * cnt / ttl_cnt, 2) as prct
    from t1
    join t2
      on t1.field = t2.field
    order by cnt desc

  distro_field_group: |
    -- distro_field_group {field}
    with t1 as (
      select
        '{field}' as field,
        {group_expr} as group_exp,
        {field},        
        count(*) cnt
      from `{schema}`.`{table}`
      group by {field}, {group_expr}
      order by count(*) desc
    )
    , t2 as (
      select
        '{field}' as field,
        count(*) ttl_cnt
      from `{schema}`.`{table}`
    )
    select
      '{table}' as table_nm,
      t1.field,
      t1.group_exp,
      {field} as value,
      cnt,
      round(100.0 * cnt / ttl_cnt, 2) as prct
    from t1
    join t2
      on t1.field = t2.field
    order by cnt desc

  distro_field_date: |
    -- distro_field_date {field}
    with t1 as (
        select
          '{field}' as field,
          year({field}) as year,
          month({field}) as month,
          count(*) cnt
        from `{schema}`.`{table}`
        group by year({field}), month({field})
        order by year({field}), month({field})
      )
      , t2 as (
        select '{field}' as field, count(*) ttl_cnt
        from `{schema}`.`{table}`
      )
      select 
        '{schema}' as schema_nm,
        '{table}' as table_nm,
        t1.field,
        t1.year,
        t1.month,
        cnt,
        round(100.0 * cnt / ttl_cnt, 2) as prct
      from t1
      join t2
        on t1.field = t2.field
      order by t1.year, t1.month

  distro_field_date_wide: |
    -- distro_field_date {table}
    select
      '{date_field}' as date_field,
      year({date_field}) as year,
      month({date_field}) as month,
      {columns_sql}
    from `{schema}`.`{table}`
    {where_clause}
    group by year({date_field}), month({date_field})
    order by year({date_field}), month({date_field})

  test_pk: |
    select
      '`{schema}`.`{table}`' as table_nm,
      '{fields_exp}' as expression,
      case when count(*) = count(distinct {fields_exp}) then 'PASS' else 'FAIL' end as pk_result,
      count(*) as tot_cnt,
      count(distinct {fields_exp}) as expression_cnt,
      count(*) - count(distinct {fields_exp}) as delta_cnt,
      sum(case when {fields_exp} is null then 1 else 0 end) as null_cnt
    from `{schema}`.`{table}`
    {where_clause}

routine:
  number_min_max: |
    select
      count(*) as tot_cnt,
      count({field}) as field_cnt,
      min({field}) as min_val,
      max({field}) as max_val
    from `{schema}`.`{table}`

  number_trunc_min_max: |
    select
      {fields}
    from `{schema}`.`{table}`
    where {where}
      (({partition_col_trunc} >= '{min_val}'
      and {partition_col_trunc} <= '{max_val}')
      {or_null})

  date_trunc_uniques: |
    select
      {partition_col_trunc} as day_field,
      count(*) cnt
    from `{schema}`.`{table}`
    {where}
    group by {partition_col_trunc}
    order by {partition_col_trunc}

  number_trunc_uniques: |
    select
      {partition_col_trunc} as trunc_field,
      count(*) cnt
    from `{schema}`.`{table}`
    {where}
    group by {partition_col_trunc}
    order by {partition_col_trunc}

function:
  replace: replace({string_expr}, {to_replace}, {replacement})
  str_utf8: '{ field }'
  cast_to_text: 'cast({field} as mediumtext)'
  cast_to_integer: 'cast({field} as signed)'
  fill_cnt_field: count({field}) as cnt_{field}
  fill_rate_field: round(100.0 * count({field}) / count(*), 2) as prct_{field}
  sleep: select sleep({seconds})
  checksum_decimal: 'abs(truncate({field}, 0))'
  checksum_datetime: cast((UNIX_TIMESTAMP({field}) * 1000000) as UNSIGNED)
  checksum_boolean: '{field}'

variable:
  # MySQL default is delete_insert since it doesn't support native MERGE
  default_merge_strategy: delete_insert
  bind_string: "?"
  quote_char: '`'
  ddl_col: 1
  batch_rows: 500
  bool_as: integer
  max_string_type: mediumtext
  max_string_length: 16777215
  max_column_length: 64

error_filter:
  table_not_exist: exist

native_type_map:
  bigint: bigint
  binary: binary
  bit: binary
  blob: text
  char: string
  date: date
  datetime: datetime
  decimal: decimal
  double: float
  enum: string
  float: float
  geomcollection: string
  geometry: string
  geometrycollection: string
  int: integer
  json: json
  linestring: string
  longblob: text
  longtext: text
  mediumblob: text
  mediumint: integer
  mediumtext: text
  multilinestring: string
  multipoint: string
  multipolygon: string
  point: string
  polygon: string
  set: string
  smallint: smallint
  text: text
  time: time
  timestamp: timestamp
  tinyblob: text
  tinyint: bool
  tinytext: text
  "unsigned bigint": "decimal(28,0)"
  "unsigned int": bigint
  "unsigned mediumint": bigint
  "unsigned smallint": integer
  "unsigned tinyint": integer
  varbinary: binary
  varchar: text
  year: string

general_type_map:
  bigint: bigint
  binary: varbinary
  bool: tinyint
  date: date
  datetime: "datetime(6)"
  decimal: "decimal(,)"
  float: double
  integer: integer
  json: json
  smallint: smallint
  string: "varchar()"
  text: mediumtext
  time: "varchar()"
  timestamp: "datetime(6)"
  timestampz: "datetime(6)"
  timez: "varchar()"
  uuid: "varchar(36)"

# Schema migration: default value translation between native and generalized forms
default_value_map:
  to_general:
    "CURRENT_TIMESTAMP": "current_timestamp"
    "current_timestamp()": "current_timestamp"
    "now()": "current_timestamp"
    "CURRENT_DATE": "current_date"
    "curdate()": "current_date"
    "CURRENT_TIME": "current_time"
    "curtime()": "current_time"
    "uuid()": "uuid()"
    "1": "true"
    "0": "false"
    "b'1'": "true"
    "b'0'": "false"
  from_general:
    "current_timestamp": "CURRENT_TIMESTAMP(6)"
    "current_timestamp_utc": "UTC_TIMESTAMP(6)"
    "current_date": "CURRENT_DATE"
    "current_time": "CURRENT_TIME"
    "uuid()": "UUID()"
    "true": "1"
    "false": "0"
    "null": "NULL"
