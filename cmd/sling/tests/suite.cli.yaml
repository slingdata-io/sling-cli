- id: 1
  name: Simple sling command
  run: sling
  output_contains:
    - Slings data from a data source to a data target.

- id: 2
  name: Run sling command
  run: sling run
  output_contains:
    - Execute a run

- id: 3
  name: Run sling with Excel source
  run: sling run --src-stream file://core/dbio/filesys/test/test.excel2.xlsx --tgt-object file://test.xlsx
  rows: 1317
  output_contains:
    - wrote 1317 rows

- id: 4
  name: Run sling with CSV source and POSTGRES target
  run: 'cat cmd/sling/tests/files/test1.1.csv | sling run --tgt-conn POSTGRES --tgt-object public.my_table --mode full-refresh'
  rows: 18

- id: 5
  name: Run sling with CSV source and POSTGRES target
  run: 'sling run --src-stream file://cmd/sling/tests/files/test1.1.csv --tgt-conn POSTGRES --tgt-object public.my_table --mode full-refresh'
  rows: 18
  after: [4]

- id: 6
  name: Run sling with CSV source and MSSQL target
  run: 'sling run --src-stream file://cmd/sling/tests/files/test1.1.csv --tgt-conn MSSQL --tgt-object dbo.my_table --mode full-refresh --tgt-options ''use_bulk: false'''
  rows: 18

- id: 7
  name: Run sling with CSV source and custom options
  run: 'sling run --src-stream file://cmd/sling/tests/files/test4.csv --src-options ''{ delimiter: "|", escape: "\\" }'' --stdout > /dev/null'
  rows: 4

- id: 8
  name: Run sling with gzipped CSV source and POSTGRES target
  run: 'cat cmd/sling/tests/files/test1.1.csv.gz | sling run --tgt-conn POSTGRES --tgt-object public.my_table1 --mode full-refresh'
  rows: 18

- id: 9
  name: Run sling with gzipped CSV source and MYSQL target
  run: 'sling run --src-stream ''file://cmd/sling/tests/files/test1.1.csv.gz'' --tgt-conn MYSQL --tgt-object mysql.my_table --mode full-refresh --tgt-options ''use_bulk: false'''
  rows: 18

- id: 10
  name: Run sling with JSON source and POSTGRES target
  run: 'cat cmd/sling/tests/files/test3.json | sling run --src-options "flatten: true" --tgt-conn POSTGRES --tgt-object public.my_table2 --tgt-options ''use_bulk: false'' --mode full-refresh'
  rows: 1

- id: 11
  name: Run sling with JSON source and POSTGRES target
  run: 'sling run --src-stream ''file://cmd/sling/tests/files/test3.json''  --src-options "flatten: true" --tgt-conn POSTGRES --tgt-object public.my_table3 --tgt-options ''use_bulk: false'' --mode full-refresh'
  rows: 1

- id: 12
  name: Run sling with CSV source and no header
  run: 'sling run --src-stream ''file://cmd/sling/tests/files/test6.csv'' --stdout -d --src-options ''{ header: false }'' > /dev/null'
  rows: 2

- id: 13
  name: Run sling with echo input and empty allowed
  run: 'echo ''a,b,c'' | SLING_ALLOW_EMPTY=true sling  run --tgt-object file:///tmp/test.csv'
  rows: 0
  bytes: 6
  output_contains:
    - execution succeeded

- id: 14
  name: Run sling with POSTGRES source and CSV output
  run: 'sling run --src-conn POSTGRES --src-stream public.my_table --stdout > /tmp/my_table.csv'
  rows: 18
  after: [5]

- id: 15
  name: Run sling with POSTGRES source and CSV target
  run: 'sling run --src-conn POSTGRES --src-stream public.my_table --tgt-object file:///tmp/my_table.csv'
  rows: 18
  after: [5]

- id: 16
  name: Run sling with POSTGRES source and select columns
  run: 'sling run --src-conn POSTGRES --src-stream public.my_table --stdout --select ''id,email'' -l 2'
  rows: 2
  after: [5]
  output_contains:
    - 'id,email'

- id: 17
  name: Run sling with POSTGRES source and exclude columns
  run: 'sling run --src-conn POSTGRES --src-stream public.my_table --stdout --select ''-id'' -l 2'
  rows: 2
  after: [5]
  output_contains:
    - 'first_name,last_name,email,target,create_dt'

- id: 18
  name: Run sling with gzipped CSV source and POSTGRES target with ignore existing
  run: 'cat cmd/sling/tests/files/test1.1.csv.gz | sling run --tgt-conn POSTGRES --tgt-object public.my_table --mode full-refresh --tgt-options ''ignore_existing: true'''
  rows: 0
  after: [17]
  output_contains:
    - execution succeeded

- id: 19
  name: Run sling with POSTGRES source and CSV target with ignore existing
  run: 'sling run --src-conn POSTGRES --src-stream public.my_table --tgt-object file:///tmp/my_table.csv --tgt-options ''ignore_existing: true'''
  rows: 0
  after: [18]
  output_contains:
    - execution succeeded

- id: 20
  name: Run sling with binary CSV source and POSTGRES target
  run: 'sling run --src-stream file://cmd/sling/tests/files/binary/test.bytes.csv --tgt-conn postgres --tgt-object public.my_table_bytes'
  rows: 1

- id: 21
  name: Execute SQL command on POSTGRES
  run: 'sling conns exec postgres "select 1 from "postgres"."public"."my_table_bytes" where byte_val::bytea::text like ''%89504e470d0a1a0a0000000d%''"'
  rows: 1
  after: [20]
  output_contains:
    - "1"

- id: 22
  name: Run sling with JSON source and custom columns
  run: SLING_STREAM_URL_COLUMN=true SLING_ROW_NUM_COLUMN=true sling run --src-stream file://core/dbio/filesys/test/test1/json --tgt-conn postgres --tgt-object public.many_jsons --mode full-refresh
  env:
    SLING_LOADED_AT_COLUMN: 'false'
  rows: 2019

- id: 23
  name: Execute SQL command to select distinct stream URL
  run: 'sling conns exec postgres "select distinct _sling_stream_url from public.many_jsons"'
  rows: 4
  after: [22]
  output_contains:
    - _SLING_STREAM_URL

- id: 24
  name: Execute SQL command to select stream URL by row number
  run: 'sling conns exec postgres "select _sling_stream_url from public.many_jsons where _sling_row_num = 18" # should show different file names'
  rows: 3
  after: [23]
  output_contains:
    - _SLING_STREAM_URL

- id: 25
  name: Execute SQL command to check column names
  run: 'sling conns exec postgres "select column_name from information_schema.columns where table_schema = ''public'' and table_name = ''many_jsons'' and column_name like ''_sling%''" # should not have _sling_loaded_at'
  rows: 2
  after: [24]
  output_contains:
    - _sling_row_num
    - _sling_stream_url

- id: 26
  name: Run sling with JSON source and timestamp column
  run: 'SLING_LOADED_AT_COLUMN=''timestamp'' sling run --src-stream file://core/dbio/filesys/test/test1/json --tgt-conn postgres --tgt-object public.many_jsons --mode full-refresh'
  rows: 2019
  after: [25]

- id: 27
  name: Execute SQL command to check data type of timestamp column
  run: 'sling conns exec postgres "select data_type from information_schema.columns where table_schema = ''public'' and table_name = ''many_jsons'' and column_name = ''_sling_loaded_at'' and data_type like ''timestamp%''" # _sling_loaded_at should be a timestamp'
  rows: 1
  after: [26]
  output_contains:
    - timestamp with

- id: 28
  name: Test POSTGRES connection
  run: sling conns test POSTGRES
  output_contains:
    - 'success!'

- id: 29
  name: Execute SQL command to count rows in POSTGRES table
  run: 'sling conns exec POSTGRES ''select count(1) from public.my_table'''
  rows: 1
  after: [18]
  output_contains:
    - 18

- id: 30
  name: Discover POSTGRES connections
  run: sling conns discover POSTGRES
  output_contains:
    - information_schema

- id: 31
  name: Discover POSTGRES connections with schema filter
  run: 'sling conns discover POSTGRES -s ''public.*'''
  output_contains:
    - information_schema

- id: 32
  name: Discover local connections
  run: 'sling conns discover local -p ''.'''
  output_contains:
    - directory

- id: 33
  name: Discover Prometheus connections with columns
  run: 'sling conns discover prometheus --columns'
  fails: gauge

- id: 34
  name: Run sling with Prometheus source and custom query
  run: >
    sling run --src-conn prometheus --src-stream 'sum(go_gc_duration_seconds) by (job, instance, quantile) # {"start": "now-2M"}' --stdout  -d
  output_contains:
    - quantile

- id: 35
  name: Run sling with replication configuration 05
  run: 'sling run -r cmd/sling/tests/replications/r.05.yaml'
  streams: 12

- id: 36
  name: Run sling with replication configuration 05 and streams
  run: 'sling run -r cmd/sling/tests/replications/r.05.yaml --streams ''s3://ocral/mlo.community.test/channels.json,s3://ocral/mlo.community.test/random/'''
  streams: 2
  after: [35]

- id: 37
  name: Run sling with replication configuration 06
  run: 'sling run -r cmd/sling/tests/replications/r.06.yaml'
  streams: 3

- id: 38
  name: Run sling with replication configuration 07
  run: sling run -r cmd/sling/tests/replications/r.07.yaml
  streams: 15
  group: sqlite

- id: 39
  name: Run sling with replication configuration 08
  group: sqlite
  run: 'sling run -r cmd/sling/tests/replications/r.08.yaml'
  streams: 4

- id: 40
  name: Run sling with replication configuration 09 and constraints
  run: '# sling run -r cmd/sling/tests/replications/r.09.yaml'
  group: sqlite
  streams: '>1'
  fails: 2

- id: 41
  name: Run sling with replication configuration 09
  run: 'sling run -d -r cmd/sling/tests/replications/r.09.yaml'
  group: sqlite
  streams: '>1'
  output_contains:
    - 'from "public"."my_table_bytes"'
    - RUNNING

- id: 42
  name: Run sling with replication configuration 09 and tags
  group: sqlite
  run: 'sling run -r cmd/sling/tests/replications/r.09.yaml --streams tag:my_table'
  streams: 3

- id: 43
  name: Run sling with replication configuration 10
  run: 'sling run -d -r cmd/sling/tests/replications/r.10.yaml'
  rows: 1018
  streams: 1
  output_contains:
    - 'singleFile=true'

- id: 44
  name: Run sling with replication configuration 11 and year parameter
  run: 'sling run -r cmd/sling/tests/replications/r.11.yaml'
  env:
    YEAR: 2005
  streams: 2
  output_contains:
    - test1k/2005

- id: 45
  name: Run sling with replication configuration 12
  run: 'sling run -r cmd/sling/tests/replications/r.12.yaml'
  streams: 1

- id: 46
  name: Run sling with replication configuration 15
  run: 'sling run -r cmd/sling/tests/replications/r.15.yaml # iceberg & delta'
  streams: 3
  output_contains:
    - 999 rows
    - 'aws_s3 -> postgres | '
    - 'test/parquet/test1.parquet'

- id: 48
  after: [46]
  name: Run sling with replication configuration 14
  run: 'sling run -d -r cmd/sling/tests/replications/r.14.yaml'
  streams: 5

- id: 49
  name: Run sling with replication configuration 14 (specific streams)
  run: 'sling run -r cmd/sling/tests/replications/r.14.yaml --streams cmd/sling/tests/files/test1.csv,cmd/sling/tests/files/test1.upsert.csv # file incremental. Second run should have no new rows'
  after: [48]
  rows: 0

- id: 50
  name: Run sling with replication configuration 16
  run: 'sling run -r cmd/sling/tests/replications/r.16.yaml'
  rows: 90
  streams: 1

- id: 51
  name: Run sling with task configuration
  run: 'sling run -c cmd/sling/tests/task.yaml'
  rows: 24

- id: 52
  name: Run sling with Parquet source
  run: 'sling run --src-stream ''file://cmd/sling/tests/files/parquet'' --stdout > /dev/null'
  rows: 1018

- id: 53
  name: Run sling with empty input
  run: 'echo '''' | sling run --stdout'
  rows: 0
  output_contains:
    - execution succeeded

- id: 54
  name: Run sling with CSV source and single quote
  run: 'sling run --src-conn LOCAL --src-stream file://cmd/sling/tests/files/test7.csv --src-options ''{ delimiter: "|", quote: "''\''''", escape: "\\" }'' --stdout > /dev/null'
  rows: 3

- id: 55
  name: Run sling with CSV source and $symbol quote
  run: 'sling run --src-conn LOCAL --src-stream file://cmd/sling/tests/files/test8.csv --src-options ''{ delimiter: "|", quote: "$", escape: "\\" }'' --stdout > /dev/null'
  rows: 3

- id: 56
  name: 'Run sling with direct insert full-refresh'
  run: 'SLING_DIRECT_INSERT=true sling run --src-conn postgres --src-stream public.test1k_postgres_pg --tgt-conn mysql --tgt-object ''mysql.public_test1k_postgres_pg'' --mode full-refresh'
  rows: '>10'
  streams: '>1'
  output_contains:
    - streaming data (direct insert)

- id: 57
  name: Run sling with incremental (delete missing soft)
  run: 'sling run -d --src-conn postgres --src-stream ''select * from public.test1k_postgres_pg where {incremental_where_cond} limit 900'' --tgt-conn mysql --tgt-object ''mysql.public_test1k_postgres_pg'' --mode incremental --primary-key id --update-key create_dt --tgt-options ''{ delete_missing: soft }'''
  rows: 0
  after: [56]
  output_contains:
    - and not exists (

- id: 58
  name: Run sling with incremental (delete missing hard)
  run: 'sling run -d --src-conn postgres --src-stream ''select * from public.test1k_postgres_pg where {incremental_where_cond} limit 900'' --tgt-conn mysql --tgt-object ''mysql.public_test1k_postgres_pg'' --mode incremental --primary-key id --update-key create_dt --tgt-options ''{ delete_missing: hard }'''
  rows: 0
  after: [57]
  output_contains:
    - and not exists (

- id: 59
  name: Run sling writing to partitioned parquet (local)
  run: |
    rm -rf /tmp/sling/output8
    
    sling run --src-stream file://cmd/sling/tests/files/test1.csv --tgt-object 'file:///tmp/sling/output8/{part_year}/{part_month}' -d --tgt-options '{ format: parquet }' --update-key create_dt
    
    ls -l /tmp/sling/output8
  rows: 1000
  output_contains:
    - partition_by (
    - 'create_dt_year=2018'

- id: 60
  name: Run sling writing to partitioned parquet (aws)
  run: 'FORMAT=parquet sling run -d -r cmd/sling/tests/replications/r.17.yaml --mode full-refresh'
  rows: 1002
  output_contains:
    - partition_by (

- id: 61
  name: Run sling with incremental writing to partitioned parquet (aws)
  run: 'FORMAT=parquet sling run -d -r cmd/sling/tests/replications/r.17.yaml'
  rows: 40
  after: [60]
  output_contains:
    - partition_by (

- id: 62
  name: Run sling writing to partitioned csv (aws)
  run: 'FORMAT=csv sling run -d -r cmd/sling/tests/replications/r.17.yaml --mode full-refresh'
  rows: 1002
  after: [61]
  output_contains:
    - partition_by (

- id: 63
  name: Run sling project init
  run: sling project init
  output_contains:
    - .sling.json

- id: 64
  name: Run sling project status
  run: sling project status
  output_contains:
    - PROJECT NAME

- id: 65
  name: Run sling project jobs
  run: sling project jobs
  output_contains:
    - manage project jobs

- id: 66
  name: Run sling project jobs list
  run: sling project jobs list
  output_contains:
    - FILE NAME

- id: 67
  name: 'Run sling hooks & source partitioned (backfill)'
  run: 'sling run -r cmd/sling/tests/replications/r.19.yaml -d --mode backfill --range 2018-01-01,2019-05-01'
  after: [61, 46, 48]
  env:
    RESET: 'true'
  rows: 551
  output_contains:
    - 'executed hook "start-03-delete" (type: delete)'
    - 'writing incremental state (value => 2019-06-01'

- id: 68
  name: 'Run sling hooks & source partitioned (incremental)'
  run: 'sling run -r cmd/sling/tests/replications/r.19.yaml -d --streams ''test1k_postgres_pg_parquet'''
  rows: '>78'
  after: [67, 46, 48]
  output_contains:
    - 'skipped hook "start-03-delete"'
    - 'hook (inspect_file_check) failed => check failure'
    - 'writing incremental state (value => 2019-07-01'

- id: 69
  name: Run sling pipeline 01
  run: 'sling run -p cmd/sling/tests/pipelines/p.01.yaml -d'
  after: [68]
  output_contains:
    - 'executed step "step-03-replication"'
    - 'key1 => value-from-pipeline'
    - 'key2 => value-from-replication'

- id: 70
  name: Run sling chunking
  run: |
    THREADS=5 MODE=full-refresh sling run -r $YAML_FILE -d
    THREADS=6 MODE=truncate     sling run -r $YAML_FILE -d
    THREADS=1 MODE=backfill     sling run -r $YAML_FILE -d
    SLING_STREAM_CNT=18 THREADS=2 MODE=incremental  sling run -r $YAML_FILE -d

    SLING_STREAM_CNT=2 MODE=full-refresh sling run -r $YAML_EXPR_FILE -d
    SLING_STREAM_CNT=2 MODE=truncate     sling run -r $YAML_EXPR_FILE -d
    SLING_STREAM_CNT=2 MODE=incremental  sling run -r $YAML_EXPR_FILE -d
  streams: 33
  group: duckdb
  env:
    YAML_FILE: cmd/sling/tests/replications/r.20.chunking.yaml
    YAML_EXPR_FILE: cmd/sling/tests/replications/r.20.chunking.expr.yaml
  output_contains:
    - '"update_dt" >= ''2018-11-21'
    - '"id" >= 601 and "id" <= 800'
    - TEST1K_SQLSERVER_PG_003
    - TEST1K_SNOWFLAKE_PG_004
    - ("date" >= '2020-09-26' and "date" <= '2021-04-23') # test1k_duckdb_pg, chunk_count: 4
    - ("id" >= 502 and "id" <= 668)  # test1k_ducklake_pg, chunk_count: 6
    # - ("update_dt" >= '2019-11-16 20:39:39' # test1k_clickhouse_pg, incremental
    - ("id" >= 836  # test1k_ducklake_pg, incremental
    - '"id" <= 1002 /* custom-sql */' # test1k_ducklake_pg_sql, incremental
    - '"update_dt" <= ''2019-11-16 20:39:39'' /* custom-sql */' # test1k_duckdb_pg_sql, incremental
    - ((mod(abs(hashtext(coalesce(first_name, ''))), 2)) = 0) # test1k_mysql_pg, chunk_expr
    - id <= 500 and ("id" >= 1 and "id" <= 500) # test1k_mysql_pg_sql

- id: 71
  name: Run sling pipeline 02
  run: 'sling run -p cmd/sling/tests/pipelines/p.02.yaml'
  output_contains:
    - state.copy_sftp_azure.bytes_written    # ensures a file was copied from SFTP to AZURE
    - state.copy_s3_azure.bytes_written      # ensures a file was copied from S3 to AZURE

- id: 72
  name: Run sling to test column casing
  run: 'sling run -d -r cmd/sling/tests/replications/r.21.yaml'
  streams: 1
  output_contains:
    - test1k_clickhouse_pg_first_name

- id: 73
  name: Run sling mysql bit
  run: 'sling run -d -r cmd/sling/tests/replications/r.22.mysql_bit_type.yaml'
  streams: 1
  output_contains:
    - execution succeeded

- id: 74
  name: Run sling iceberg_r2 insert
  run: |
    sling run -d -r cmd/sling/tests/replications/r.23.iceberg_write.yaml
    sling run --src-conn $TARGET --src-stream public.test1k_mariadb_pg --stdout -l 5
  env:
    TARGET: iceberg_r2
  streams: 2
  output_contains:
    - committed iceberg snapshot
    - count_result => {"count_star":1001}
    - inserted 967 rows
    
    # part of stdout output
    - writing to target stream (stdout)
    - wrote 5 rows

- id: 75
  name: Run sling iceberg_s3 insert
  run: |
    sling run -d -r cmd/sling/tests/replications/r.23.iceberg_write.yaml
    sling run --src-conn $TARGET --src-stream public.test1k_mariadb_pg --stdout -l 5
  env:
    TARGET: iceberg_s3
  after: [74]
  streams: 2
  output_contains:
    - committed iceberg snapshot
    - count_result => {"count_star":1001}
    - inserted 967 rows
    
    # part of stdout output
    - writing to target stream (stdout)
    - wrote 5 rows

# - id: 76
#   name: Run sling iceberg_lakekeeper insert
#   run: |
#     sling run -d -r cmd/sling/tests/replications/r.23.iceberg_write.yaml
#     sling run --src-conn $TARGET --src-stream public.test1k_mariadb_pg --stdout -l 5
#   env:
#     TARGET: iceberg_lakekeeper
#   streams: 2
#   output_contains:
#     - committed iceberg snapshot
#     - count_result => {"count_star":1001}
#     - inserted 967 rows
    
#     # part of stdout output
#     - writing to target stream (stdout)
#     - wrote 5 rows

- id: 77
  name: Run sling iceberg_sql insert
  run: |
    sling run -d -r cmd/sling/tests/replications/r.23.iceberg_write.yaml
    sling run --src-conn $TARGET --src-stream public.test1k_mariadb_pg --stdout -l 5
  env:
    TARGET: iceberg_sql
  after: [75]
  streams: 2
  output_contains:
    - committed iceberg snapshot
    - inserted 967 rows
    
    # part of stdout output
    - writing to target stream (stdout)
    - wrote 5 rows

- id: 78
  name: Run sling iceberg_glue insert
  run: |
    sling run -d -r cmd/sling/tests/replications/r.23.iceberg_write.yaml
    sling run --src-conn $TARGET --src-stream public.test1k_mariadb_pg --stdout -l 5
  env:
    TARGET: iceberg_glue
  after: [77]
  streams: 2
  output_contains:
    - committed iceberg snapshot
    - inserted 967 rows
    
    # part of stdout output
    - writing to target stream (stdout)
    - wrote 5 rows

# - id: 79
#   name: Run sling iceberg_gcp insert
#   run: |
#     sling run -d -r cmd/sling/tests/replications/r.23.iceberg_write.yaml
#     sling run --src-conn $TARGET --src-stream public.test1k_mariadb_pg --stdout -l 5
#   streams: 2
#   env:
#     TARGET: iceberg_gcp
#   output_contains:
#     - committed iceberg snapshot
#     - inserted 967 rows
    
#     # part of stdout output
#     - writing to target stream (stdout)
#     - wrote 5 rows

- id: 80
  name: Prometheus buffer fix test
  run: sling run -d -r cmd/sling/tests/replications/r.25.prometheus_buffer.yaml
  streams: 1
  min_rows: 1
  output_contains:
    - 'using range'
    - "changing column type via transform for 'timestamp': bigint => string"
    - "changing column type via transform for 'value': decimal => string"
    - "'timestamp':'text', 'value':'text'" # the timestamp & value columns were successfully casted to string

- id: 81
  name: Prometheus issue 551 (https://github.com/slingdata-io/sling-cli/issues/551)
  run: sling run -d -r cmd/sling/tests/replications/r.26.prometheus_issue551.yaml
  streams: 1
  output_contains:
    - 'using chunked streaming'

- id: 82
  name: Pipeline copy single file from list result (fix for duplicate filename issue)
  run: sling run -d -p cmd/sling/tests/pipelines/p.03.test_copy_fix_demonstration.yaml
  output_contains:
    - 'copying single file from'
    - 'Copy result:'
    - 'Output files:'
    - 'listing path: /tmp/demo_file_1.csv'
  output_does_not_contain:
    - 'listing path: /tmp/remote'

# - id: 83
#   name: Delta R2 secret creation test (https://github.com/slingdata-io/sling-cli/issues/564)
#   run: sling run -d -r cmd/sling/tests/replications/r.27.delta_r2.yaml
#   err: true  # We expect this to fail because the delta file doesn't exist
#   output_contains:
#     - "delta_scan('r2://"  # Verifies the URL was transformed from s3:// to r2://
#     - 'No files in log segment'
#   # This test validates that the R2 secret is created with ACCOUNT_ID
#   # It will fail at data reading stage, but that proves the secret was created correctly

- id: 84
  name: Oracle NUMBER scale fix (https://github.com/slingdata-io/sling-cli/issues/584)
  run: sling run -r cmd/sling/tests/replications/r.28.oracle_number_scale.yaml
  env:
    SCHEMA: ORACLE
    SOURCE: ORACLE
    TARGET: SNOWFLAKE
  output_contains:
    - 'execution succeeded'

- id: 85
  name: Oracle NUMBER scale fix (https://github.com/slingdata-io/sling-cli/issues/584)
  run: sling run -r cmd/sling/tests/replications/r.28.oracle_number_scale.yaml
  env:
    SCHEMA: ORACLE
    SOURCE: ORACLE
    TARGET: POSTGRES
  output_contains:
    - 'execution succeeded'

- id: 86
  name: MSSQL to parquet overwrite fix (https://github.com/slingdata-io/sling-cli/issues/588)
  run: sling run -r cmd/sling/tests/replications/r.29.mssql_parquet_overwrite.yaml
  output_contains:
    - 'execution succeeded'

- id: 87
  name: MSSQL to Azure Storage multiple files fix (https://github.com/slingdata-io/sling-cli/issues/586)
  run: sling run -r cmd/sling/tests/replications/r.30.mssql_azure_multiple_files.yaml
  output_contains:
    - 'execution succeeded'

- id: 88
  name: Test timestamp to string casting without quotes
  run: sling run -r cmd/sling/tests/replications/r.31.timestamp_string_quotes.yaml --debug
  rows: 3
  group: duckdb
  output_contains:
    - wrote 3 rows
    - execution succeeded

- id: 89
  name: Test inspect hook for database tables and files
  run: sling run -p cmd/sling/tests/pipelines/p.04.test_inspect_hook.yaml
  output_contains:
    - 'Database table inspection results:'
    - 'S3 file inspection results:'
    - 'Local file inspection results:'
    - 'Local directory inspection results:'
    - 'Path: sling-cli-tests/inspect_test.txt'
    - 'FDQN: "public"."test_inspect_table"'
    - 'Path: /tmp/sling/inspect_test/test_file.json'
    - 'URI: file:///tmp/sling/inspect_test/'
    - 'Is Dir: true'

- id: 90
  name: starrocks empty_as_null
  run: sling run -d -r cmd/sling/tests/replications/r.24.starrocks_empty_as_null.yaml
  streams: 1
  rows: 5
  output_contains:
    - 'store.result'
  output_does_not_contain:
    - 'WARN: Using INSERT mode'

- id: 91
  name: Test encoding source options (latin1, windows1252, utf8_bom) and target encoding
  run: |
    sling run -d -r cmd/sling/tests/replications/r.32.encoding_test.yaml
    sling run -d -r cmd/sling/tests/replications/r.33.encoding_target_test.yaml
  output_contains:
    - 'Latin1 results:'
    - 'José'
    - 'Café manager'
    - 'Windows1252 results:'
    - 'UTF-8 BOM results:'
    - '张三'
    - '김철수'
    - 'wrote 3 rows'
    - 'execution succeeded'

- id: 92
  name: Test encoding garbled output when not specified
  run: sling run -d -r cmd/sling/tests/replications/r.34.encoding_garbled_test.yaml
  output_contains:
    - 'SUCCESS: Correct output contains proper special characters'
    - 'SUCCESS: Garbled output has corrupted special characters as expected'
    - 'execution succeeded'

- id: 93
  name: Test transform functions
  run: sling run -d -r cmd/sling/tests/replications/r.35.transform_functions_test.yaml
  output_contains:
    - 'Transform Functions Test Results'
    - 'execution succeeded'
  output_does_not_contain:
    - "did find find transform with params named: 'now'"

- id: 94
  name: Test mongo column casing selection
  run: sling run -d -r cmd/sling/tests/replications/r.36.mongo_snowflake_select.yaml
  output_contains:
    - '"projection":[{"Key":"id","Value":1},{"Key":"first_name","Value":1},{"Key":"last_name","Value":1}]}'
  output_does_not_contain:
     # before, it would upper-case the columns upon column listing
    - '"projection":[{"Key":"ID","Value":1},{"Key":"FIRST_NAME","Value":1},{"Key":"LAST_NAME","Value":1}]}}'

- id: 95
  name: Test decimal to string casting with proper varchar length (https://github.com/slingdata-io/sling-cli/issues/599)
  run: sling run -d -r cmd/sling/tests/replications/r.37.decimal_to_string_cast.yaml
  output_contains:
    - 'Result from Snowflake:'
    - '"100.000"'
    - '"-999.999"'
    - '"0.00001"'

- id: 96
  name: Test extra files deletion (https://github.com/slingdata-io/sling-cli/issues/614)
  run: sling run -d -p cmd/sling/tests/pipelines/p.05.extra_parquet_files_deletion.yaml
  output_does_not_contain:
    - step (extra_files_check) failed

- id: 97
  name: Test to ensure snowflake compression for upload
  run: sling run -d -r cmd/sling/tests/replications/r.38.snowflake_compression_test.yaml
  output_contains:
    - compression=zstd
    - compression=gzip

- id: 98
  name: Test SLING_LOADED_AT column type when casting all columns to string
  run: |
    sling run -d -r cmd/sling/tests/replications/r.39.sling_loaded_at_string_cast.yaml
    sling run -d -r cmd/sling/tests/replications/r.40.sling_loaded_at_csv_string_cast.yaml
  output_contains:
    - 'SUCCESS: _SLING_LOADED_AT is correctly kept as TIMESTAMP even when all columns are cast to string (Postgres source)'
    - "SUCCESS: _SLING_LOADED_AT is correctly kept as TIMESTAMP even when all columns are cast to string (CSV source)"

# - id: 99
#   name: Test empty dataset exporting with Parquet (https://github.com/slingdata-io/sling-cli/issues/598)
#   run: |
#     sling run -d -r cmd/sling/tests/replications/r.41.redshit_empty_parquet.yaml
#   output_contains:
#     - no data or records found in stream

- id: 100
  name: Test excluding column when exporting to Parquet https://github.com/slingdata-io/sling-cli/issues/607)
  run: |
    sling run -d -r cmd/sling/tests/replications/r.42.mssql_exclude_column_issue607.yaml
  output_contains:
    - 'extra_col column count in parquet file: 0'

- id: 101
  name: Test Azure Table incremental mode to PostgreSQL
  run: |
    MODE=full-refresh sling run -d -r cmd/sling/tests/replications/r.43.azuretable_incremental_postgres.yaml --limit 5
    MODE=incremental sling run -d -r cmd/sling/tests/replications/r.43.azuretable_incremental_postgres.yaml
  streams: 1
  output_contains:
    - 'Loaded 5 rows into public.test1k_azuretable_pg'
    - 'inserted 213 rows'
    - 'rows into public.test1k_azuretable_pg'
    - 'id, first_name, email, create_dt, Timestamp'

- id: 102
  name: Test staged transforms with MySQL source and PostgreSQL target
  run: sling run -d -r cmd/sling/tests/replications/r.44.staged_transforms_test.yaml
  streams: 3
  rows: 15  # 3 * 5 = 15
  output_contains:
    - 'Staged Transforms Test Results'
    - 'execution succeeded'

- id: 103
  name: Test Exasol large CSV file ingestion (100K rows)
  run: sling run -d -r cmd/sling/tests/replications/r.45.exasol_large_csv.yaml
  streams: 1
  rows: 100000
  output_contains:
    - 'inserted 100000 rows'
    - 'Row count: 100000'
    - 'execution succeeded'

- id: 104
  name: Test PostgreSQL & BigQury TIME datatype preservation
  run: |
    sling run -d -r cmd/sling/tests/replications/r.46.postgres_time_datatype.yaml
    sling run -d -r cmd/sling/tests/replications/r.48.bigquery_time_datatype.yaml
  output_contains:
    - '| time without time zone |'
    - '| time with time zone    |'
    - | # pg output
      +----+----------+--------------------+----------------+
      | ID | TIME_COL | TIME_PRECISION_COL | TIME_TZ_COL    |
      +----+----------+--------------------+----------------+
      | 1  | 08:30:45 | 08:30:45.123       | 08:30:45+02:00 |
      | 2  | 12:15:30 | 12:15:30.456       | 12:15:30-05:00 |
      | 3  | 23:59:59 | 23:59:59.789       | 23:59:59+00:00 |
      +----+----------+--------------------+----------------+
    - | # bq output
      +----+----------+--------------------+----------------+
      | ID | TIME_COL | TIME_PRECISION_COL | TIME_TZ_COL    |
      +----+----------+--------------------+----------------+
      | 1  | 08:30:45 | 08:30:45.123       | 08:31:45+02:00 |
      | 2  | 12:15:30 | 12:15:30.456       | 12:11:30-05:00 |
      | 3  | 23:59:59 | 23:59:59.789       | 23:51:59+00:00 |
      +----+----------+--------------------+----------------+

- id: 105
  name: Test MSSQL uniqueidentifier datatype preservation
  run: |
    sling conns exec mssql file://cmd/sling/tests/replications/r.47.mssql_uniqueidentifier.sql
    SLING_CLI_TOKEN='' sling run -d -r cmd/sling/tests/replications/r.47.mssql_uniqueidentifier.yaml
    sling run -d -r cmd/sling/tests/replications/r.47.mssql_uniqueidentifier.postgres.yaml
    # sling run -d -r cmd/sling/tests/replications/r.47.fabric_uniqueidentifier.yaml
  output_contains:
    - "execution succeeded"
    - "DROP TABLE public.unique_identifier_test2"

- id: 106
  name: Test MSSQL to BigQuery BIGNUMERIC conversion for high precision/scale decimals
  run: sling run -d -r cmd/sling/tests/replications/r.49.mssql_bigquery_bignumeric.yaml
  streams: 1
  rows: 3
  output_contains:
    - "Successfully retrieved column information from BigQuery"
    - "The BIGNUMERIC conversion logic is working correctly!"
    - "All 3 test records should be transferred"
    - "Data integrity verified - high precision decimal values transferred successfully"
    - "execution succeeded"

- id: 107
  name: Test CSV to BigQuery BIGNUMERIC conversion for high scale decimal values
  run: sling run -d -r cmd/sling/tests/replications/r.50.csv_bigquery_bignumeric.yaml
  streams: 1
  rows: 10
  output_contains:
    - "high_scale_1 correctly converted to BIGNUMERIC"
    - "high_scale_2 correctly converted to BIGNUMERIC"
    - "edge_scale_10 (scale=10 > 9) correctly converted to BIGNUMERIC"
    - "very_high_scale correctly converted to BIGNUMERIC"
    - "normal_scale correctly remains as NUMERIC"
    - "small_decimal correctly remains as NUMERIC"
    - "All 10 CSV rows loaded successfully"
    - "High scale decimal value 1 preserved accurately"
    - "Negative high scale decimal preserved accurately"
    - "Very small high scale decimal preserved accurately"
    - "Edge scale=10 BIGNUMERIC value preserved accurately"
    - "Normal scale NUMERIC value preserved accurately"
    - "Small decimal NUMERIC value preserved accurately"
    - "Edge scale=9 NUMERIC value preserved accurately"
    - "Very high scale BIGNUMERIC value preserved accurately"
    - "CSV to BigQuery BIGNUMERIC conversion test completed successfully!"
    - "execution succeeded"

- id: 108
  name: Run XML to PostgreSQL import test
  run: 'sling run -r cmd/sling/tests/replications/r.51.xml_postgres_import.yaml'
  streams: 1
  rows: 5
  output_contains:
    - 'execution succeeded'
    - '✓ All 5 XML records imported successfully'
    - '✓ Name field imported correctly'
    - '✓ Email field imported correctly'
    - '✓ Age field imported correctly'
    - '✓ Salary field imported correctly'
    - '✓ XML to PostgreSQL import test completed successfully!'

# - id: 109
#   name: Test API -- Sling Platform
#   run: sling run -r cmd/sling/tests/replications/apis/r.sling_platform.yaml -d
#   streams: 3
#   output_contains:
#     - 'execution succeeded'
    
#     # test iterations
#     - '/connection/list?test-value=1'
#     - '/connection/list?test-value=2'
#     - '/connection/list?test-value=3'
#     - '/connection/list?test-value=4'
#     - '/execution/list?filters=%7B%22period'

- id: 110
  name: Test API -- Sling Stripe
  run: |
    SLING_STATE=postgres/sling_state.stripe sling run -r cmd/sling/tests/replications/apis/r.stripe.yaml

    # test without SLING_STATE, to use DB incremental value
    sling run -r cmd/sling/tests/replications/apis/r.stripe.yaml --streams customer -d
  output_contains:
    - 'execution succeeded'
    - 'fetched incremental value from DB'

- id: 111
  name: Run AWS S3 file splitting test
  run: 'sling run -r cmd/sling/tests/replications/r.52.parquet_file_splitting.yaml'
  env:
    MY_TARGET: aws_s3
  streams: 1
  rows: 1000
  output_contains:
    - 'execution succeeded'

- id: 112
  name: Run AZURE_STORAGE file splitting test
  run: 'sling run -r cmd/sling/tests/replications/r.52.parquet_file_splitting.yaml'
  env:
    MY_TARGET: AZURE_STORAGE
  streams: 1
  rows: 1000
  output_contains:
    - 'execution succeeded'

- id: 113
  name: Run GOOGLE_STORAGE file splitting test
  run: 'sling run -r cmd/sling/tests/replications/r.52.parquet_file_splitting.yaml'
  env:
    MY_TARGET: GOOGLE_STORAGE
  streams: 1
  rows: 1000
  output_contains:
    - 'execution succeeded'

- id: 114
  name: Run local file splitting test
  run: 'sling run -r cmd/sling/tests/replications/r.52.parquet_file_splitting.yaml'
  env:
    MY_TARGET: local
  streams: 1
  rows: 1000
  output_contains:
    - 'execution succeeded'

# Test Prometheus with no data (should not error)
- id: 115
  run: 'sling run -r cmd/sling/tests/replications/r.53.prometheus_incremental.yaml'
  env:
    SLING_ALLOW_EMPTY: 'TRUE'
  conns:
    - prometheus
    - postgres
  streams: 1
  rows: 0
  output_contains:
    - 'execution succeeded'

# Test MongoDB ObjectID filtering
- id: 116
  name: MongoDB filter test
  run: |
    sling run -r cmd/sling/tests/replications/r.54.mongo_filter.yaml
    sling run -r cmd/sling/tests/replications/r.54.mongo_objectid_filter.yaml

- id: 117
  name: JSON to Snowflake nested import test
  run: 'sling run -d -r cmd/sling/tests/replications/r.55.json_snowflake_nested.yaml'

# Test SQL Server money type to StarRocks
- id: 118
  name: SQL Server money type to StarRocks
  run: 'sling run -d -r cmd/sling/tests/replications/r.56.mssql_starrocks_money.yaml'
  conns:
    - mssql
    - starrocks
  output_contains:
    - 'execution succeeded'
    - 'Row-by-row comparison'

- id: 119
  name: Run replication test for RunState properties
  run: 'sling run -r cmd/sling/tests/replications/r.57.run_state_test.yaml'
  streams: 2
  rows: 10
  output_contains:
    - run.total_bytes = 948
    - run.total_rows = 10
    - run.status = success
    - '   id = public_run_state_test'

- id: 120
  name: Test PostgreSQL mixed encoding character handling
  run: 'sling run -d -r cmd/sling/tests/replications/r.58.postgres_invalid_utf8.yaml'
  rows: 10
  output_contains:
    - 'execution succeeded'

- id: 121
  name: Test MSSQL to PostgreSQL encoding handling
  run: 'sling run -d -r cmd/sling/tests/replications/r.59.mssql_postgres_encoding.yaml'
  rows: 10
  after: [120]
  output_contains:
    - 'execution succeeded'

- id: 122
  name: Test MSSQL to PostgreSQL collations handling
  run: 'sling run -d -r cmd/sling/tests/replications/r.60.mssql_postgres_collations.yaml'
  rows: 5
  output_contains:
    - 'execution succeeded'

- id: 123
  name: Test MSSQL identity columns with incremental mode (Issue #657)
  run: 'sling run -d -r cmd/sling/tests/replications/r.61.mssql_identity_incremental.yaml'
  output_contains:
    - 'execution succeeded'

# with add_new_columns=false, we should WARN instead of erroring
- id: 124
  name: Test MSSQL to PostgreSQL with add_new_columns=false
  run: 'sling run -d -r cmd/sling/tests/replications/r.62.mssql_postgres_add_columns_false.yaml'
  output_contains:
    - source field 'age' is missing in target table

# Test PostgreSQL to BigQuery with all data types using CSV and Parquet format
- id: 125
  name: Test PostgreSQL to BigQuery with all data types
  run: 'sling run -d -r cmd/sling/tests/replications/r.63.postgres_bigquery_all_types.yaml'

# Test SQL Server time(7) to Oracle varchar conversion (Issue #662)
- id: 126
  name: Test MSSQL time(7) to Oracle conversion (Issue #662)
  run: 'sling run -d -r cmd/sling/tests/replications/r.64.mssql_oracle_time.yaml'
  env:
    SCHEMA: ORACLE
  conns:
    - mssql
    - oracle
  streams: 1
  rows: 4
  output_contains:
    - 'execution succeeded'
    - 'Row count: 4'

# Test large CSV to Postgres to Parquet (WriteDataflowReadyViaDuckDB hang test)
- id: 127
  name: Test large CSV import to Postgres (100k rows)
  run: |
    sling run -d -r cmd/sling/tests/replications/r.65.large_csv_postgres_parquet.part1.yaml
    sling run -d -r cmd/sling/tests/replications/r.65.large_csv_postgres_parquet.part2.yaml
  conns:
    - postgres
  output_contains:
    - 'CSV file created with 100,000'
    - 'execution succeeded'
    - '✓ All 100,000 CSV records imported successfully into Postgres'

# Test MSSQL to Databricks with uppercase table name (Issue #664)
# - id: 128
#   name: Test MSSQL to Databricks with uppercase table name (Issue #664)
#   run: 'sling run -d -r cmd/sling/tests/replications/r.66.mssql_databricks_uppercase_table.yaml'
#   output_contains:
#     - 'execution succeeded'

# Test JSON camelCase to PostgreSQL with column_casing (should only create snake_case columns)
- id: 129
  name: Test JSON camelCase to PostgreSQL column casing (reproduce duplicate columns bug)
  after: [117]
  run: 'sling run -d -r cmd/sling/tests/replications/r.67.json_postgres_column_casing.yaml'
  streams: 1
  rows: 3
  output_contains:
    - 'execution succeeded'
    - '✓ All 3 JSON records imported successfully'
    - '✓ Correct number of unique columns (5: user_id, first_name, last_name, email_address, _sling_loaded_at)'
    - '✓ Data correctly imported into snake_case columns'

- id: 130
  name: Test HTTP write_to
  run: 'sling run -d -p cmd/sling/tests/pipelines/p.07.http.yaml'
  output_contains:
    - 'successfully executed step "step-08-check" (type: check)'

- id: 131
  name: Test Routine Hook
  run: 'sling run -d -p cmd/sling/tests/pipelines/p.07.routine.yaml'
  output_contains:
    - '=== Test 1: Simple logging routine ==='
    - 'Starting routine: test_logging'
    - 'Parameter value: hello_from_pipeline from cmd/sling/tests/pipelines'
    - 'Environment variable: environment_test_value'
    - '=== Test 2: Data validation routine ==='
    - 'Validating data for table: public.test_routine_table'
    - 'Table public.test_routine_table has'
    - '=== Test 3: File operations routine ==='
    - 'Processing file: local//tmp/routine_test_file.txt'
    - 'routine (missing_param_test) requires params that were not provided: ["test_param"]'
    - '=== Test 4: HTTP fetch routine ==='
    - 'Fetching data from: https://www.omdbapi.com'
    - '=== Test 5: Nested routine test ==='
    - 'Starting validation suite'
    - '=== Test 6: Conditional routine execution ==='
    - '=== All routine tests completed successfully ==='

- id: 132
  name: Test hooks in defaults with wildcards
  run: 'sling run -d -r cmd/sling/tests/replications/r.68.defaults_hooks_wildcard.yaml'
  output_contains:
    - 'wrote 14 rows to file:///tmp/wildcard_test1_upsert'
    - 'wrote 1000 rows to file:///tmp/exact_test2.csv'
    - 'PRE HOOK EXECUTED for stream: cmd/sling/tests/files/test2.csv'
    - 'POST HOOK EXECUTED for stream: cmd/sling/tests/files/test2.csv'
    - 'PRE HOOK EXECUTED for stream: cmd/sling/tests/files/test1.1.csv'
    - 'POST HOOK EXECUTED for stream: cmd/sling/tests/files/test1.1.csv'
    - 'PRE HOOK EXECUTED for stream: cmd/sling/tests/files/test1.2.csv'
    - 'POST HOOK EXECUTED for stream: cmd/sling/tests/files/test1.2.csv'

- id: 133
  name: Test SSH Command Hook
  run: 'sling run -d -p cmd/sling/tests/pipelines/p.08.ssh_command.yaml'
  output_contains:
    - '=== Test 1: Basic SSH command execution ==='
    - 'SSH hostname output:'
    - '=== Test 2: SSH command with working directory ==='
    - 'SSH pwd output:'
    - '=== Test 3: SSH command with environment variables ==='
    - 'SSH env output:'
    - 'hello_from_ssh'
    - '=== Test 4: SSH command array format ==='
    - 'SSH array command output:'
    - 'test array command'
    - '=== Test 5: SSH command with timeout ==='
    - 'SSH timeout command output:'
    - 'completed'
    - '=== Test 6: SSH command creating and reading a file ==='
    - 'SSH file content:'
    - 'test content'
    - '=== All SSH command tests completed successfully ==='

- id: 134
  name: Test MSSQL to Postgres decimal precision
  run: 'sling run -d -r cmd/sling/tests/replications/r.69.mssql_postgres_decimal.yaml'
  streams: 1

- id: 135
  name: Test MSSQL to Postgres decimal cast_as (float and string)
  run: 'sling run -d -r cmd/sling/tests/replications/r.70.mssql_postgres_decimal_cast_as.yaml'
  streams: 2

- id: 136
  name: Test nested JSON to SQLite with CLI flags
  group: sqlite
  run: |
    bash cmd/sling/tests/replications/r.71.json_sqlite_nested.sh

    sling run -d --src-conn LOCAL --tgt-conn SQLITE --src-stream "file:///tmp/test_nested_json/*.json" --tgt-object "main.test_nested_json_cli" --mode full-refresh --src-options '{ format: json, flatten: true }'

- id: 137
  name: Test nested JSON to SQLite with replication config (pre-defined columns)
  run: 'sling run -d -r cmd/sling/tests/replications/r.71.json_sqlite_nested.yaml'
  group: sqlite
  err: true
  output_contains:
   - cannot be assigned to zero-based row

- id: 138
  name: Test wildcard stream with disabled table exclusion
  run: |
    sling conns exec mysql "
      DROP TABLE IF EXISTS testing_1;
      DROP TABLE IF EXISTS testing_2;
      DROP TABLE IF EXISTS testing_3;
      CREATE TABLE testing_1 (id INT PRIMARY KEY, name VARCHAR(50));
      INSERT INTO testing_1 (id, name) VALUES (1, 'table_one');
      CREATE TABLE testing_2 (id INT PRIMARY KEY, name VARCHAR(50));
      INSERT INTO testing_2 (id, name) VALUES (2, 'table_two');
      CREATE TABLE testing_3 (id INT PRIMARY KEY, name VARCHAR(50));
      INSERT INTO testing_3 (id, name) VALUES (3, 'table_three');
    "
    sling run -d -r cmd/sling/tests/replications/r.72.wildcard_disabled.yaml
  output_contains:
    - '✅ SUCCESS: Wildcard with disabled stream works correctly'

- id: 139
  name: 'Test DuckDB reading Parquet with large strings from S3 (issue #668)'
  run: 'sling run -d -r cmd/sling/tests/replications/r.73.duckdb_large_parquet.yaml'
  output_contains:
    - '✓ All 100 rows with large strings loaded successfully'
    - '✓ Large string values preserved (exactly 65,330 chars)'

- id: 140
  name: 'Test ClickHouse boolean to string conversion (issue #626)'
  run: |
    TARGET=clickhouse sling run -d -r cmd/sling/tests/replications/r.74.clickhouse_bool_string.yaml
    TARGET=clickhouse_top sling run -d -r cmd/sling/tests/replications/r.74.clickhouse_bool_string.yaml
  output_contains:
    - '✅ SUCCESS: Boolean values correctly converted to strings in ClickHouse'
    - '✓ clickhouse_test2.is_active has boolean type'
    - '✅ SUCCESS: Boolean values correctly stored as boolean type in clickhouse_test2'

- id: 141
  name: 'Test {fields} placeholder with select parameter (issue #669)'
  run: 'sling run -d -r cmd/sling/tests/replications/r.75.fields_placeholder_select.yaml'
  output_contains:
    - SELECT "id", "name"

- id: 142
  name: 'Test columns casting to string for DDL (issue #651)'
  run: 'sling run -d -r cmd/sling/tests/replications/r.32.columns_cast_to_string.yaml'
  output_contains:
    - 'Column type for date_col: character varying'
    - casting column 'date_col' as 'string'

- id: 143
  name: Test HTTP POST hooks with JSON payloads
  run: 'sling run -p cmd/sling/tests/pipelines/p.09.http_post.yaml'
  output_contains:
    - '✓ Test 1 PASSED: Basic map payload correctly converted to JSON'
    - '✓ Test 2 PASSED: Template variables correctly substituted in map payload'
    - '✓ Test 3 PASSED: Nested structures correctly marshaled to JSON'
    - '✓ Test 4 PASSED: Array payload correctly marshaled with variable substitution'
    - '✓ Test 5 PASSED: String in JSON payload'
    - '✓ Test 6 PASSED: Special characters properly handled in JSON payload'
    - '✓ Test 7 PASSED: Edge cases handled correctly'
    - 'All tests completed successfully!'
    
- id: 144
  name: Test Directory run
  run: |
    sling run cmd/sling/tests/replications/many/nested/p.command.yaml
    sling run --directory cmd/sling/tests/replications/many/nested
    sling run cmd/sling/tests/replications/many/
  output_contains:
    - 'p.log.yaml (pipeline)'
    - 'r.local.yaml (replication)'

- id: 145
  name: Test thread state sync (SLING_THREADS)
  run: sling run -r cmd/sling/tests/replications/r.76.thread_state_sync.yaml -d
  output_contains:
    - 'Stream 1: total_rows=5, status=success'
    - 'Stream 2: total_rows=5, status=success'
    - 'Stream 3: total_rows=5, status=success'
    - 'Stream 4: total_rows=5, status=success'

# Requires ADBC_POSTGRES connection with PostgreSQL ADBC driver installed
# Install driver: dbc install postgresql
- id: 146
  name: Test ADBC PostgreSQL connection
  run: sling run -r cmd/sling/tests/replications/r.77.adbc_postgres.yaml -d
  output_contains:
    - 'Testing ADBC PostgreSQL connection'
    - 'Read CSV data:'
    - '1,test'
    - 'ADBC test completed successfully'
    - 'wrote 1 rows'

# Requires ADBC_SQLSERVER connection with SQL Server ADBC driver installed
# Install driver: dbc install mssql
- id: 147
  name: Test ADBC SQL Server connection
  run: sling run -r cmd/sling/tests/replications/r.78.adbc_sqlserver.yaml -d
  output_contains:
    - 'Testing ADBC SQL Server connection'
    - 'Read CSV data:'
    - '1,test'
    - 'ADBC SQL Server test completed successfully'
    - 'wrote 1 rows'

# Tests write mode with BulkImportFlow using adbc.IngestStream
- id: 148
  name: Test ADBC write
  run: |
    TARGET=POSTGRES_ADBC SCHEMA=public sling run -r cmd/sling/tests/replications/r.79.adbc_write.yaml -d
    TARGET=MSSQL_ADBC SCHEMA=dbo sling run -r cmd/sling/tests/replications/r.79.adbc_write.yaml -d
  output_contains:
    - 'ADBC write test completed successfully'
    - 'inserted 3 rows'

# Tests ADBC write with large data (100k rows)
- id: 149
  name: Test ADBC write large
  run: |
    # TARGET=POSTGRES_ADBC SCHEMA=public sling run -r cmd/sling/tests/replications/r.80.adbc_write_large.yaml -d
    TARGET=MSSQL_ADBC SCHEMA=dbo sling run -r cmd/sling/tests/replications/r.80.adbc_write_large.yaml -d
    # TARGET=MYSQL_ADBC SCHEMA=mysql sling run -r cmd/sling/tests/replications/r.80.adbc_write_large.yaml -d
  output_contains:
    - 'ADBC write large test completed successfully'

- id: 150
  name: 'Test MSSQL boolean to string conversion'
  run: |
    sling run --debug -r cmd/sling/tests/replications/r.81.mssql_bool_string.yaml
  output_contains:
    - 'SUCCESS: Boolean values correctly converted to strings in MSSQL'
    - 'mssql_test2.is_active has boolean type'
    - 'SUCCESS: Boolean values correctly stored as boolean type in mssql_test2'

- id: 151
  name: 'Test boolean column_typing cast_as (integer/string)'
  run: |
    sling run --debug -r cmd/sling/tests/replications/r.82.mssql_postgres_boolean_cast_as.yaml
  output_contains:
    - 'SUCCESS: Boolean values correctly cast to integer in bool_test1'
    - 'SUCCESS: Boolean values correctly cast to string in bool_test2'
    - 'SUCCESS: Boolean values correctly stored as boolean in bool_test3'

- id: 152
  name: 'Test MSSQL BIT to MySQL integer/varchar conversion'
  run: |
    sling run --debug -r cmd/sling/tests/replications/r.83.mssql_mysql_bool.yaml
  output_contains:
    - 'SUCCESS: BIT values correctly converted to integers in bool_testing_1'
    - 'SUCCESS: BIT values correctly converted to strings in bool_testing_2'

- id: 153
  name: 'Test ODBC data extraction from MSSQL_ODBC to PostgreSQL'
  run: |
    sling run --debug -r cmd/sling/tests/replications/r.84.mssql_odbc_extraction.yaml

- id: 154
  name: 'Test MSSQL BIT type casting across multiple targets via pipeline'
  run: |
    sling run --debug -p cmd/sling/tests/pipelines/p.10.bool_cast_multi_target.yaml
  output_contains:
    # verify order of streams
    - '[1 / 4] running stream dbo.bool_cast_source'
    - '[2 / 4] running stream bool_cast_varchar'
    - '[3 / 4] running stream bool_cast_bool'
    - '[4 / 4] running stream bool_cast_integer'
    - 'SUCCESS: BIT correctly cast to INTEGER for postgres'
    - 'SUCCESS: BIT correctly cast to VARCHAR for postgres'
    - 'SUCCESS: BIT correctly cast to BOOLEAN for postgres'
    - 'SUCCESS: BIT correctly cast to INTEGER for mysql'
    - 'SUCCESS: BIT correctly cast to VARCHAR for mysql'
    - 'SUCCESS: BIT correctly cast to BOOLEAN for mysql'
    - 'SUCCESS: BIT correctly cast to INTEGER for mariadb'
    - 'SUCCESS: BIT correctly cast to VARCHAR for mariadb'
    - 'SUCCESS: BIT correctly cast to BOOLEAN for mariadb'

- id: 155
  name: 'Test SLING_SYNCED_AT_COLUMN with soft delete (MSSQL to Postgres)'
  run: |
    sling run --debug -r cmd/sling/tests/replications/r.85.mssql_postgres_synced_at.yaml
  output_contains:
    - 'execution succeeded'
    - "SUCCESS: All 10 rows have _sling_synced_op='I' after full-refresh"
    - 'SUCCESS: _sling_synced_at column exists'
    - 'SUCCESS: _sling_deleted_at column does NOT exist'
    - 'SUCCESS: synced_at_test1 has 2 distinct _sling_synced_at values'
    - 'SUCCESS: _sling_synced_op column exists'
    - "SUCCESS: IDs 9,10 have _sling_synced_op='D' (soft deleted)"
    - "SUCCESS: IDs 1-8 have _sling_synced_op='U' (updated)"

- id: 156
  name: Test mixed-case record key references in transforms (MySQL to local parquet)
  run: 'sling run -d -r cmd/sling/tests/replications/r.86.record_key_casing.yaml'
  streams: 1
  rows: 3
  output_contains:
    - 'SUCCESS: All 3 rows exported successfully with mixed-case column transform'
    - 'SUCCESS: true_changed_at column was computed correctly'
    - 'execution succeeded'

- id: 157
  name: Test mixed-case record key references in transforms (MySQL to BigQuery)
  run: 'sling run -d -r cmd/sling/tests/replications/r.87.record_key_casing_bigquery.yaml'
  after: [156]
  streams: 1
  rows: 3
  output_contains:
    - 'SUCCESS: All 3 rows exported successfully with mixed-case column transform (BigQuery)'
    - 'SUCCESS: true_changed_at column was computed correctly (BigQuery)'
    - 'execution succeeded'

# # Test PostGIS (PostgreSQL) to GeoJSON as target with geometry column name "geom"
# - id: 150
#   name: Test PostGIS (PostgreSQL) to GeoJSON as target
#   run: 'sling run -d -r cmd/sling/tests/replications/r.68.postgres_geom_column_definition_geojson.yaml && test -f /tmp/test_geojson_output.geojson && cat /tmp/test_geojson_output.geojson'
#   rows: 3
#   output_contains:
#     - 'execution succeeded'
#     - '"type":"FeatureCollection"'
#     - '"geometry":{"type":"Point","coordinates":[9.09425263416477,53.4920035631827]}'
#     - '"properties":{"id":1,"name":"Point 1"}'

# # Test PostGIS (PostgreSQL) to GeoJSON as target with default geometry column name "geometry"
# - id: 151
#   name: Test PostGIS (PostgreSQL) to GeoJSON as target
#   run: 'sling run -d -r cmd/sling/tests/replications/r.69.postgres_geom_column_default_geojson.yaml && test -f /tmp/test_geojson_output.geojson && cat /tmp/test_geojson_output.geojson'
#   rows: 3
#   output_contains:
#     - 'execution succeeded'
#     - '"type":"FeatureCollection"'
#     - '"geometry":{"type":"Point","coordinates":[9.09425263416477,53.4920035631827]}'
#     - '"properties":{"id":1,"name":"Point 1"}'

# GitHub Issue #694: table_ddl with user-defined PRIMARY KEY and WITH clause
- id: 158
  name: 'Test custom table_ddl with PRIMARY KEY and WITH clause (MSSQL)'
  run: 'sling run -d -r cmd/sling/tests/replications/r.88.table_ddl_with_clause.yaml'
  streams: 2
  rows: 2
  output_contains:
    - 'execution succeeded'
    - 'Primary key columns:'
    - 'Primary key columns (WITH clause table):'
    - 'Compression info:'

# GitHub Issue #678: definition-only mode
- id: 159
  name: 'Test definition-only mode creates table without data (Postgres to MSSQL)'
  run: 'sling run -d -r cmd/sling/tests/replications/r.89.definition_only_db.yaml'
  streams: 1
  rows: 0
  output_contains:
    - 'created table definition'
    - 'execution succeeded'
    - 'SUCCESS: Table definition created with correct schema and 0 rows'

- id: 160
  name: 'Test definition-only mode creates parquet file without data'
  run: 'sling run -d -r cmd/sling/tests/replications/r.90.definition_only_file.yaml'
  streams: 1
  rows: 0
  output_contains:
    - 'where 1=0'
    - 'execution succeeded'
    - 'SUCCESS: Parquet file definition created with correct schema'

- id: 161
  name: 'Test definition-only mode fails for CSV file target'
  run: 'sling run --src-conn POSTGRES --src-stream "select 1 as a" --tgt-object file:///tmp/test_def_only.csv --mode definition-only'
  err: true
  output_contains:
    - 'only supports parquet or arrow formats'

- id: 162
  name: 'Test definition-only mode from parquet file source to database'
  run: 'sling run -d -r cmd/sling/tests/replications/r.91.definition_only_file_source.yaml'
  output_contains:
    - 'execution succeeded'
    - 'SUCCESS: Table created from parquet file source with schema and 0 rows'

# Test Oracle XMLTYPE column transfer to BigQuery (hang issue)
- id: 163
  name: 'Test Oracle XMLTYPE column to BigQuery transfer (hang issue)'
  run: 'sling run -d -r cmd/sling/tests/replications/r.92.oracle_xmltype_bigquery.yaml'
  streams: 1
  rows: 3
  output_contains:
    - "using text since type 'xmltype' not mapped"
    - 'execution succeeded'
    - 'SUCCESS: Oracle XMLTYPE to BigQuery transfer completed without hanging!'

# Test MySQL LoadDataLocal using RegisterReaderHandler pattern
- id: 164
  name: 'Test MySQL LoadDataLocal with native Go driver'
  run: 'sling run -d -r cmd/sling/tests/replications/r.93.mysql_load_data_local.yaml'
  streams: 1
  rows: 18
  output_contains:
    - 'execution succeeded'
    - 'SUCCESS: MySQL LoadDataLocal test passed'

# Test MySQL LoadDataLocal NULL handling with checksums
- id: 165
  name: 'Test MySQL LoadDataLocal NULL handling'
  run: 'sling run -d -r cmd/sling/tests/replications/r.94.mysql_load_data_local_nulls.yaml'
  streams: 1
  rows: 53
  output_contains:
    - 'execution succeeded'
    - 'SUCCESS: MySQL LoadDataLocal NULL handling test passed'

# Test column renaming via select option (comprehensive tests)
- id: 166
  name: 'Test column renaming via select (comprehensive)'
  run: 'sling run -d -r cmd/sling/tests/replications/r.95.select_column_rename.yaml'
  streams: 6
  output_contains:
    - 'execution succeeded'
    - 'Test 1 - Basic rename columns'
    - 'Test 2 - Exclude columns'
    - 'Test 3 - Wildcard exclude columns'
    - 'Test 4 - Mixed select columns'
    - 'Test 5 - Table with rename columns'
    - 'Test 6 - SQL {fields} placeholder columns'

# Test S3 multi-bucket access with single connection (different buckets in stream URIs and files key)
- id: 167
  name: 'Test S3 multi-bucket access with single connection'
  run: 'sling run -d -r cmd/sling/tests/replications/r.96.s3_multi_bucket.yaml'
  streams: 3
  output_contains:
    - 'SUCCESS: S3 multi-bucket access with single connection works correctly'

# Test delete_missing with transforms doesn't fail (transforms should be skipped during delete detection)
- id: 168
  name: 'Test delete_missing with transforms (transforms skipped during delete detection)'
  run: 'sling run -d -r cmd/sling/tests/replications/r.97.delete_missing_with_transforms.yaml'
  streams: 2
  output_contains:
    - 'execution succeeded'
    - 'SUCCESS: computed_date column exists from transform'
    - 'SUCCESS: IDs 9,10 are soft-deleted correctly'

# Test chunking with {stream_table} variable in object field (bug fix)
# Issue: When using chunking + custom SQL, {stream_table} was not expanded in temp table names
- id: 169
  name: 'Test chunking with {stream_table} variable expansion'
  run: 'sling run -d -r cmd/sling/tests/replications/r.98.chunking_stream_table_var.yaml'
  streams: 3
  output_contains:
    - 'execution succeeded'
    - 'SUCCESS: chunking with {stream_table} variable works'

# Test MySQL LoadDataLocal escape character handling (backslash bug fix)
# Issue: LOAD DATA with ESCAPED BY '\\' misparses rows with backslash sequences (\N, \", etc.)
# Fix: Changed to ESCAPED BY '' to use RFC 4180 double-quote escaping which matches Go's csv.Writer
- id: 170
  name: 'Test MySQL LoadDataLocal escape character handling'
  run: 'sling run -d -r cmd/sling/tests/replications/r.99.mysql_load_data_escape_bug.yaml'
  streams: 1
  rows: 25
  output_contains:
    - 'execution succeeded'
    - 'SUCCESS: MySQL LoadDataLocal escape handling test passed'

# # Test configurable merge strategies (update_insert, delete_insert, insert, update)
# # Feature: Allow users to specify merge_strategy in target_options to control how incremental data is merged
- id: 171
  name: 'Test merge_strategy target option'
  run: 'sling run -d -r cmd/sling/tests/replications/r.100.merge_strategy.yaml'
  streams: 4
  output_contains:
    - 'execution succeeded'
    - 'update_insert count'
    - 'delete_insert count'
    - 'update count'
    - 'insert count'

# Test constraint validation with value IN expressions
# Bug: Constraints fail to abort replication when invalid data is present
# The constraint should detect invalid rating value and abort the replication
- id: 172
  name: 'Test constraint validation with value IN expression (abort mode)'
  run: 'sling run -d -r cmd/sling/tests/replications/r.101.constraints_test.yaml'
  err: true
  output_contains:
    - 'constraint failure'
    - 'SUCCESS: Constraint validation in ABORT mode works correctly'

# Test constraint validation with 21+ failures in abort mode
# Bug fix: After 20 constraint failures, errors should still cause abortion
- id: 173
  name: 'Test constraint validation with 21+ failures (abort mode)'
  run: 'sling run -d -r cmd/sling/tests/replications/r.102.constraints_21plus_bug.yaml'
  err: true
  output_contains:
    - 'constraint failure'
    - 'execution failed'

# Test constraint validation with 25+ failures in warn mode
# Bug fix: After 20 warnings, remaining failures should still be counted and logged
# In warn mode, invalid data IS allowed through but warnings should continue
- id: 174
  name: 'Test constraint validation with 25+ failures (warn mode)'
  run: 'sling run -d -r cmd/sling/tests/replications/r.103.constraints_warn_21plus_bug.yaml'
  streams: 1
  rows: 30
  output_contains:
    - 'execution succeeded'
    - 'logging limit reached'
    - 'SUCCESS: Constraint validation in WARN mode works correctly'

# Test delete_missing with source_where and target_where for scoped deletion (soft delete)
# Allows efficient delete detection on large tables by limiting comparisons to a subset of data
- id: 175
  name: 'Test delete_missing soft with source_where/target_where (scoped deletion)'
  run: 'sling run -d -r cmd/sling/tests/replications/r.104.delete_missing_where.yaml'
  streams: 2
  output_contains:
    - 'execution succeeded'
    - 'SUCCESS: delete_missing with where clause correctly scoped deletion to recent data only'

# Test delete_missing hard delete with source_where and target_where for scoped deletion
- id: 176
  name: 'Test delete_missing hard with source_where/target_where (scoped deletion)'
  run: 'sling run -d -r cmd/sling/tests/replications/r.104.delete_missing_where_hard.yaml'
  streams: 2
  output_contains:
    - 'execution succeeded'
    - 'SUCCESS: hard delete with target_where correctly scoped deletion to recent data only'

# Schema Migration Comprehensive Tests (SLING_SCHEMA_MIGRATION feature)
# These tests verify all schema attributes (identity, FKs, indexes, defaults, nullable, descriptions)
# are correctly migrated from source to target databases across 6 database pairs.
# Each test uses 5 tables with streams listed in wrong order to test FK reordering.

# Test schema migration - MSSQL to PostgreSQL
- id: 177
  name: 'Schema migration comprehensive (MSSQL to Postgres)'
  run: sling run -d cmd/sling/tests/pipelines/schema_migration/p.11.sm_mssql_pg.yaml
  streams: 5
  output_contains:
    - 'FK constraints found'
    - 'Indexes found'
    - 'Identity columns found'
    - 'Column descriptions found'
    - 'Table descriptions found'
    - 'execution succeeded'
  output_does_not_contain:
    - 'could not create foreign key'

# Test schema migration - MSSQL to MySQL
- id: 178
  name: 'Schema migration comprehensive (MSSQL to MySQL)'
  run: 'sling run -d cmd/sling/tests/pipelines/schema_migration/p.12.sm_mssql_mysql.yaml'
  streams: 5
  after: [177]
  output_contains:
    - 'FK constraints found'
    - 'Indexes found'
    - 'Auto-increment columns found'
    - 'Column descriptions found'
    - 'Table descriptions found'
    - 'execution succeeded'
  output_does_not_contain:
    - 'could not create foreign key'

# Test schema migration - MSSQL to Oracle
- id: 179
  name: 'Schema migration comprehensive (MSSQL to Oracle)'
  run: 'sling run -d cmd/sling/tests/pipelines/schema_migration/p.13.sm_mssql_oracle.yaml'
  streams: 5
  after: [178]
  output_contains:
    - 'FK constraints found'
    - 'Indexes found'
    - 'Identity columns found'
    - 'Column descriptions found'
    - 'Table descriptions found'
    - 'execution succeeded'
  output_does_not_contain:
    - 'could not create foreign key'

# Test schema migration - PostgreSQL to MSSQL (reverse direction)
- id: 180
  name: 'Schema migration comprehensive (Postgres to MSSQL)'
  run: 'sling run -d cmd/sling/tests/pipelines/schema_migration/p.14.sm_pg_mssql.yaml'
  streams: 5
  output_contains:
    - 'FK constraints found'
    - 'Indexes found'
    - 'Identity columns found'
    - 'Column descriptions found'
    - 'Table descriptions found'
    - 'execution succeeded'
  output_does_not_contain:
    - 'could not create foreign key'

# Test schema migration - Oracle to MSSQL
- id: 181
  name: 'Schema migration comprehensive (Oracle to MSSQL)'
  run: 'sling run -d cmd/sling/tests/pipelines/schema_migration/p.15.sm_oracle_mssql.yaml'
  streams: 5
  after: [179]
  output_contains:
    - 'FK constraints found'
    - 'Indexes found'
    - 'Identity columns found'
    - 'Column descriptions found'
    - 'Table descriptions found'
    - 'execution succeeded'
  output_does_not_contain:
    - 'could not create foreign key'

# Test schema migration - MySQL to PostgreSQL
- id: 182
  name: 'Schema migration comprehensive (MySQL to Postgres)'
  run: 'sling run -d cmd/sling/tests/pipelines/schema_migration/p.16.sm_mysql_pg.yaml'
  streams: 5
  after: [178]
  output_contains:
    - 'FK constraints found'
    - 'Indexes found'
    - 'Identity columns found'
    - 'Column descriptions found'
    - 'Table descriptions found'
    - 'execution succeeded'
  output_does_not_contain:
    - 'could not create foreign key'

# Schema Migration Tests for Warehouse/Analytical Databases
# These tests verify schema attributes (identity, FKs, descriptions) migration
# to warehouse databases: Snowflake, Databricks, Redshift, BigQuery, DuckDB, ClickHouse

# Test schema migration - PostgreSQL to Snowflake
- id: 183
  name: 'Schema migration comprehensive (Postgres to Snowflake)'
  run: 'sling run -d cmd/sling/tests/pipelines/schema_migration/p.17.sm_pg_snowflake.yaml'
  streams: 5
  output_contains:
    - 'FK constraints found'
    - 'Identity columns found'
    - 'Column descriptions found'
    - 'Table descriptions found'
    - 'execution succeeded'
  output_does_not_contain:
    - 'could not create foreign key'

# Test schema migration - PostgreSQL to Databricks
# - id: 184
#   name: 'Schema migration comprehensive (Postgres to Databricks)'
#   run: 'sling run -d cmd/sling/tests/pipelines/schema_migration/p.18.sm_pg_databricks.yaml'
#   streams: 5
#   output_contains:
#     - 'FK constraints found'
#     - 'Column descriptions found'
#     - 'Table descriptions found'
#     - 'execution succeeded'
#   output_does_not_contain:
#     - 'could not create foreign key'

# Test schema migration - PostgreSQL to Redshift
# - id: 185
#   name: 'Schema migration comprehensive (Postgres to Redshift)'
#   run: 'sling run -d cmd/sling/tests/pipelines/schema_migration/p.19.sm_pg_redshift.yaml'
#   streams: 5
#   output_contains:
#     - 'FK constraints found'
#     - 'Identity columns found'
#     - 'Column descriptions found'
#     - 'Table descriptions found'
#     - 'execution succeeded'
#   output_does_not_contain:
#     - 'could not create foreign key'

# Test schema migration - PostgreSQL to BigQuery
- id: 186
  name: 'Schema migration comprehensive (Postgres to BigQuery)'
  run: 'sling run -d cmd/sling/tests/pipelines/schema_migration/p.20.sm_pg_bigquery.yaml'
  streams: 5
  after: [183]
  output_contains:
    - 'FK constraints found'
    - 'Column descriptions found'
    - 'Table descriptions found'
    - 'execution succeeded'
  output_does_not_contain:
    - 'could not create foreign key'

# Test schema migration - PostgreSQL to DuckDB
# Note: DuckDB doesn't support ALTER TABLE ADD FOREIGN KEY, so FK migration is not tested
- id: 187
  name: 'Schema migration comprehensive (Postgres to DuckDB)'
  run: 'sling run -d cmd/sling/tests/pipelines/schema_migration/p.21.sm_pg_duckdb.yaml'
  streams: 5
  output_contains:
    - 'PK constraints found'
    - 'execution succeeded'

# Test schema migration - PostgreSQL to ClickHouse (column comments only)
# Note: ClickHouse doesn't support ALTER TABLE for table comments
- id: 188
  name: 'Schema migration comprehensive (Postgres to ClickHouse)'
  run: 'sling run -d cmd/sling/tests/pipelines/schema_migration/p.22.sm_pg_clickhouse.yaml'
  streams: 5
  output_contains:
    - 'Column descriptions found'
    - 'execution succeeded'

# Test schema migration - SQL Server identity fix
# Tests: Identity seed/increment only returned for identity columns
- id: 189
  name: 'Schema migration SQL Server identity fix'
  run: 'sling run -d cmd/sling/tests/pipelines/schema_migration/p.23.sm_sqlserver_identity_fix.yaml'
  streams: 1
  output_contains:
    - 'Identity columns found'
    - 'Non-identity columns found'
    - 'execution succeeded'

# Test schema migration - BigQuery COLUMNS fix
# Tests: Uses INFORMATION_SCHEMA.COLUMNS instead of COLUMN_FIELD_PATHS
- id: 190
  name: 'Schema migration BigQuery columns fix'
  run: 'sling run -d cmd/sling/tests/pipelines/schema_migration/p.24.sm_bigquery_columns_fix.yaml'
  streams: 1
  output_contains:
    - 'Columns found: 6'
    - 'Column names'
    - 'execution succeeded'

# Test schema migration - MySQL indexes fix
# Tests: indexes query excludes PRIMARY key and includes uniqueness info
- id: 191
  name: 'Schema migration MySQL indexes fix'
  run: 'sling run -d cmd/sling/tests/pipelines/schema_migration/p.25.sm_mysql_indexes_fix.yaml'
  streams: 1
  output_contains:
    - 'Primary key constraints found: 1'
    - 'Indexes found'
    - 'execution succeeded'

# Test schema migration - Databricks basic fix
# Tests: Basic column migration works without identity metadata errors
# Note: Databricks connection often has timeouts - skipped by default
# - id: 192
#   name: 'Schema migration Databricks basic fix'
#   run: 'sling run -d cmd/sling/tests/pipelines/schema_migration/p.26.sm_databricks_basic_fix.yaml'
#   streams: 1
#   output_contains:
#     - 'Columns migrated: 4'
#     - 'Migrated rows: 1'
#     - 'execution succeeded'

# Test schema migration - Redshift identity fix
# Tests: Identity detection via pg_attribute.attidentity
# Note: Redshift cluster not always available - skipped by default
# - id: 193
#   name: 'Schema migration Redshift identity fix'
#   run: 'sling run -d cmd/sling/tests/pipelines/schema_migration/p.27.sm_redshift_identity_fix.yaml'
#   streams: 1
#   output_contains:
#     - 'Identity columns found'
#     - 'Non-identity columns found'
#     - 'execution succeeded'

# Test schema migration - DuckDB indexes fix
# Tests: Index extraction using expressions column
- id: 194
  name: 'Schema migration DuckDB indexes fix'
  run: 'sling run -d cmd/sling/tests/pipelines/schema_migration/p.28.sm_duckdb_fk_indexes_fix.yaml'
  streams: 1
  output_contains:
    - 'PK constraints found'
    - 'Indexes found'
    - 'execution succeeded'

# Test schema migration - ClickHouse primary key fix
# Tests: Primary key detection from system.tables
- id: 195
  name: 'Schema migration ClickHouse primary key fix'
  run: 'sling run -d cmd/sling/tests/pipelines/schema_migration/p.29.sm_clickhouse_pk_fix.yaml'
  streams: 1
  output_contains:
    - 'Primary key columns found'
    - 'Migrated rows: 1'
    - 'execution succeeded'