- id: 1
  name: Simple sling command
  run: sling
  output_contains:
    - Slings data from a data source to a data target.

- id: 2
  name: Run sling command
  run: sling run
  output_contains:
    - Execute a run

- id: 3
  name: Run sling with Excel source
  run: sling run --src-stream file://core/dbio/filesys/test/test.excel2.xlsx --tgt-object file://test.xlsx
  rows: 1317
  output_contains:
    - wrote 1317 rows

- id: 4
  name: Run sling with CSV source and POSTGRES target
  run: 'cat cmd/sling/tests/files/test1.1.csv | sling run --tgt-conn POSTGRES --tgt-object public.my_table --mode full-refresh'
  rows: 18

- id: 5
  name: Run sling with CSV source and POSTGRES target
  run: 'sling run --src-stream file://cmd/sling/tests/files/test1.1.csv --tgt-conn POSTGRES --tgt-object public.my_table --mode full-refresh'
  rows: 18

- id: 6
  name: Run sling with CSV source and MSSQL target
  run: 'sling run --src-stream file://cmd/sling/tests/files/test1.1.csv --tgt-conn MSSQL --tgt-object dbo.my_table --mode full-refresh --tgt-options ''use_bulk: false'''
  rows: 18

- id: 7
  name: Run sling with CSV source and custom options
  run: 'sling run --src-stream file://cmd/sling/tests/files/test4.csv --src-options ''{ delimiter: "|", escape: "\\" }'' --stdout > /dev/null'
  rows: 4

- id: 8
  name: Run sling with gzipped CSV source and POSTGRES target
  run: 'cat cmd/sling/tests/files/test1.1.csv.gz | sling run --tgt-conn POSTGRES --tgt-object public.my_table1 --mode full-refresh'
  rows: 18

- id: 9
  name: Run sling with gzipped CSV source and MYSQL target
  run: 'sling run --src-stream ''file://cmd/sling/tests/files/test1.1.csv.gz'' --tgt-conn MYSQL --tgt-object mysql.my_table --mode full-refresh --tgt-options ''use_bulk: false'''
  rows: 18

- id: 10
  name: Run sling with JSON source and POSTGRES target
  run: 'cat cmd/sling/tests/files/test3.json | sling run --src-options "flatten: true" --tgt-conn POSTGRES --tgt-object public.my_table2 --tgt-options ''use_bulk: false'' --mode full-refresh'
  rows: 1

- id: 11
  name: Run sling with JSON source and POSTGRES target
  run: 'sling run --src-stream ''file://cmd/sling/tests/files/test3.json''  --src-options "flatten: true" --tgt-conn POSTGRES --tgt-object public.my_table3 --tgt-options ''use_bulk: false'' --mode full-refresh'
  rows: 1

- id: 12
  name: Run sling with CSV source and no header
  run: 'sling run --src-stream ''file://cmd/sling/tests/files/test6.csv'' --stdout -d --src-options ''{ header: false }'' > /dev/null'
  rows: 2

- id: 13
  name: Run sling with echo input and empty allowed
  run: 'echo ''a,b,c'' | SLING_ALLOW_EMPTY=true sling  run --tgt-object file:///tmp/test.csv'
  rows: 0
  bytes: 6
  output_contains:
    - execution succeeded

- id: 14
  name: Run sling with POSTGRES source and CSV output
  run: 'sling run --src-conn POSTGRES --src-stream public.my_table --stdout > /tmp/my_table.csv'
  rows: 18

- id: 15
  name: Run sling with POSTGRES source and CSV target
  run: 'sling run --src-conn POSTGRES --src-stream public.my_table --tgt-object file:///tmp/my_table.csv'
  rows: 18

- id: 16
  name: Run sling with POSTGRES source and select columns
  run: 'sling run --src-conn POSTGRES --src-stream public.my_table --stdout --select ''id,email'' -l 2'
  rows: 2
  output_contains:
    - 'id,email'

- id: 17
  name: Run sling with POSTGRES source and exclude columns
  run: 'sling run --src-conn POSTGRES --src-stream public.my_table --stdout --select ''-id'' -l 2'
  rows: 2
  output_contains:
    - 'first_name,last_name,email,target,create_dt'

- id: 18
  name: Run sling with gzipped CSV source and POSTGRES target with ignore existing
  run: 'cat cmd/sling/tests/files/test1.1.csv.gz | sling run --tgt-conn POSTGRES --tgt-object public.my_table --mode full-refresh --tgt-options ''ignore_existing: true'''
  rows: 0
  output_contains:
    - execution succeeded

- id: 19
  name: Run sling with POSTGRES source and CSV target with ignore existing
  run: 'sling run --src-conn POSTGRES --src-stream public.my_table --tgt-object file:///tmp/my_table.csv --tgt-options ''ignore_existing: true'''
  rows: 0
  output_contains:
    - execution succeeded

- id: 20
  name: Run sling with binary CSV source and POSTGRES target
  run: 'sling run --src-stream file://cmd/sling/tests/files/binary/test.bytes.csv --tgt-conn postgres --tgt-object public.my_table_bytes'
  rows: 1

- id: 21
  name: Execute SQL command on POSTGRES
  run: 'sling conns exec postgres "select 1 from "postgres"."public"."my_table_bytes" where byte_val::bytea::text like ''%89504e470d0a1a0a0000000d%''"'
  rows: 1
  output_contains:
    - "1"

- id: 22
  name: Run sling with JSON source and custom columns
  run: SLING_STREAM_URL_COLUMN=true SLING_ROW_NUM_COLUMN=true sling run --src-stream file://core/dbio/filesys/test/test1/json --tgt-conn postgres --tgt-object public.many_jsons --mode full-refresh
  env:
    SLING_LOADED_AT_COLUMN: 'false'
  rows: 2019

- id: 23
  name: Execute SQL command to select distinct stream URL
  run: 'sling conns exec postgres "select distinct _sling_stream_url from public.many_jsons"'
  rows: 4
  output_contains:
    - _SLING_STREAM_URL

- id: 24
  name: Execute SQL command to select stream URL by row number
  run: 'sling conns exec postgres "select _sling_stream_url from public.many_jsons where _sling_row_num = 18" # should show different file names'
  rows: 3
  output_contains:
    - _SLING_STREAM_URL

- id: 25
  name: Execute SQL command to check column names
  run: 'sling conns exec postgres "select column_name from information_schema.columns where table_schema = ''public'' and table_name = ''many_jsons'' and column_name like ''_sling%''" # should not have _sling_loaded_at'
  rows: 2
  output_contains:
    - _sling_row_num
    - _sling_stream_url

- id: 26
  name: Run sling with JSON source and timestamp column
  run: 'SLING_LOADED_AT_COLUMN=''timestamp'' sling run --src-stream file://core/dbio/filesys/test/test1/json --tgt-conn postgres --tgt-object public.many_jsons --mode full-refresh'
  rows: 2019

- id: 27
  name: Execute SQL command to check data type of timestamp column
  run: 'sling conns exec postgres "select data_type from information_schema.columns where table_schema = ''public'' and table_name = ''many_jsons'' and column_name = ''_sling_loaded_at'' and data_type like ''timestamp%''" # _sling_loaded_at should be a timestamp'
  rows: 1
  output_contains:
    - timestamp with

- id: 28
  name: Test POSTGRES connection
  run: sling conns test POSTGRES
  output_contains:
    - 'success!'

- id: 29
  name: Execute SQL command to count rows in POSTGRES table
  run: 'sling conns exec POSTGRES ''select count(1) from public.my_table'''
  rows: 1
  output_contains:
    - 18

- id: 30
  name: Discover POSTGRES connections
  run: sling conns discover POSTGRES
  output_contains:
    - information_schema

- id: 31
  name: Discover POSTGRES connections with schema filter
  run: 'sling conns discover POSTGRES -s ''public.*'''
  output_contains:
    - information_schema

- id: 32
  name: Discover local connections
  run: 'sling conns discover local -p ''.'''
  output_contains:
    - directory

- id: 33
  name: Discover Prometheus connections with columns
  run: 'sling conns discover prometheus --columns'
  fails: gauge

- id: 34
  name: Run sling with Prometheus source and custom query
  run: >
    sling run --src-conn prometheus --src-stream 'sum(go_gc_duration_seconds) by (job, instance, quantile) # {"start": "now-2M"}' --stdout  -d
  output_contains:
    - quantile

- id: 35
  name: Run sling with replication configuration 05
  run: 'sling run -r cmd/sling/tests/replications/r.05.yaml'
  streams: 12

- id: 36
  name: Run sling with replication configuration 05 and streams
  run: 'sling run -r cmd/sling/tests/replications/r.05.yaml --streams ''s3://ocral/mlo.community.test/channels.json,s3://ocral/mlo.community.test/random/'''
  streams: 2

- id: 37
  name: Run sling with replication configuration 06
  run: 'sling run -r cmd/sling/tests/replications/r.06.yaml'
  streams: 3

- id: 38
  name: Run sling with replication configuration 07
  run: sling run -r cmd/sling/tests/replications/r.07.yaml
  streams: 15

- id: 39
  name: Run sling with replication configuration 08
  run: 'sling run -r cmd/sling/tests/replications/r.08.yaml'
  streams: 4

- id: 40
  name: Run sling with replication configuration 09 and constraints
  run: '# sling run -r cmd/sling/tests/replications/r.09.yaml'
  streams: '>1'
  fails: 2

- id: 41
  name: Run sling with replication configuration 09
  run: 'sling run -d -r cmd/sling/tests/replications/r.09.yaml'
  streams: '>1'
  output_contains:
    - 'running stream "public"."my_table_bytes"'
    - RUNNING

- id: 42
  name: Run sling with replication configuration 09 and tags
  run: 'sling run -r cmd/sling/tests/replications/r.09.yaml --streams tag:my_table'
  streams: 3

- id: 43
  name: Run sling with replication configuration 10
  run: 'sling run -d -r cmd/sling/tests/replications/r.10.yaml'
  rows: 1018
  streams: 1
  output_contains:
    - 'singleFile=true'

- id: 44
  name: Run sling with replication configuration 11 and year parameter
  run: 'sling run -r cmd/sling/tests/replications/r.11.yaml'
  env:
    YEAR: 2005
  streams: 2
  output_contains:
    - test1k/2005

- id: 45
  name: Run sling with replication configuration 12
  run: 'sling run -r cmd/sling/tests/replications/r.12.yaml'
  streams: 1

- id: 46
  name: Run sling with replication configuration 15
  run: 'sling run -r cmd/sling/tests/replications/r.15.yaml # iceberg & delta'
  streams: 3
  output_contains:
    - 999 rows
    - 'aws_s3 -> postgres | test/parquet/test1.parquet'

- id: 48
  name: Run sling with replication configuration 14
  run: 'sling run -d -r cmd/sling/tests/replications/r.14.yaml'
  streams: 5

- id: 49
  name: Run sling with replication configuration 14 (specific streams)
  run: 'sling run -r cmd/sling/tests/replications/r.14.yaml --streams cmd/sling/tests/files/test1.csv,cmd/sling/tests/files/test1.upsert.csv # file incremental. Second run should have no new rows'
  rows: 0

- id: 50
  name: Run sling with replication configuration 16
  run: 'sling run -r cmd/sling/tests/replications/r.16.yaml'
  rows: 90
  streams: 1

- id: 51
  name: Run sling with task configuration
  run: 'sling run -c cmd/sling/tests/task.yaml'
  rows: 24

- id: 52
  name: Run sling with Parquet source
  run: 'sling run --src-stream ''file://cmd/sling/tests/files/parquet'' --stdout > /dev/null'
  rows: 1018

- id: 53
  name: Run sling with empty input
  run: 'echo '''' | sling run --stdout'
  rows: 0
  output_contains:
    - execution succeeded

- id: 54
  name: Run sling with CSV source and single quote
  run: 'sling run --src-conn LOCAL --src-stream file://cmd/sling/tests/files/test7.csv --src-options ''{ delimiter: "|", quote: "''\''''", escape: "\\" }'' --stdout > /dev/null'
  rows: 3

- id: 55
  name: Run sling with CSV source and $symbol quote
  run: 'sling run --src-conn LOCAL --src-stream file://cmd/sling/tests/files/test8.csv --src-options ''{ delimiter: "|", quote: "$", escape: "\\" }'' --stdout > /dev/null'
  rows: 3

- id: 56
  name: 'Run sling with direct insert full-refresh'
  run: 'SLING_DIRECT_INSERT=true sling run --src-conn postgres --src-stream public.test1k_postgres_pg --tgt-conn mysql --tgt-object ''mysql.public_test1k_postgres_pg'' --mode full-refresh'
  rows: '>10'
  streams: '>1'
  output_contains:
    - streaming data (direct insert)

- id: 57
  name: Run sling with incremental (delete missing soft)
  run: 'sling run -d --src-conn postgres --src-stream ''select * from public.test1k_postgres_pg where {incremental_where_cond} limit 900'' --tgt-conn mysql --tgt-object ''mysql.public_test1k_postgres_pg'' --mode incremental --primary-key id --update-key create_dt --tgt-options ''{ delete_missing: soft }'''
  rows: 0
  output_contains:
    - and not exists (

- id: 58
  name: Run sling with incremental (delete missing hard)
  run: 'sling run -d --src-conn postgres --src-stream ''select * from public.test1k_postgres_pg where {incremental_where_cond} limit 900'' --tgt-conn mysql --tgt-object ''mysql.public_test1k_postgres_pg'' --mode incremental --primary-key id --update-key create_dt --tgt-options ''{ delete_missing: hard }'''
  rows: 0
  output_contains:
    - and not exists (

- id: 59
  name: Run sling writing to partitioned parquet (local)
  run: |
    rm -rf /tmp/sling/output8
    
    sling run --src-stream file://cmd/sling/tests/files/test1.csv --tgt-object 'file:///tmp/sling/output8/{part_year}/{part_month}' -d --tgt-options '{ format: parquet }' --update-key create_dt
    
    ls -l /tmp/sling/output8
  rows: 1000
  output_contains:
    - partition_by (
    - 'create_dt_year=2018'

- id: 60
  name: Run sling writing to partitioned parquet (aws)
  run: 'FORMAT=parquet sling run -d -r cmd/sling/tests/replications/r.17.yaml --mode full-refresh'
  rows: 1002
  output_contains:
    - partition_by (

- id: 61
  name: Run sling with incremental writing to partitioned parquet (aws)
  run: 'FORMAT=parquet sling run -d -r cmd/sling/tests/replications/r.17.yaml'
  rows: 40
  output_contains:
    - partition_by (

- id: 62
  name: Run sling writing to partitioned csv (aws)
  run: 'FORMAT=csv sling run -d -r cmd/sling/tests/replications/r.17.yaml --mode full-refresh'
  rows: 1002
  output_contains:
    - partition_by (

- id: 63
  name: Run sling project init
  run: sling project init
  output_contains:
    - .sling.json

- id: 64
  name: Run sling project status
  run: sling project status
  output_contains:
    - PROJECT NAME

- id: 65
  name: Run sling project jobs
  run: sling project jobs
  output_contains:
    - manage project jobs

- id: 66
  name: Run sling project jobs list
  run: sling project jobs list
  output_contains:
    - FILE NAME

- id: 67
  name: 'Run sling hooks & source partitioned (backfill)'
  run: 'sling run -r cmd/sling/tests/replications/r.19.yaml -d --mode backfill --range 2018-01-01,2019-05-01'
  env:
    RESET: 'true'
  rows: 551
  output_contains:
    - 'executed hook "start-02" (type: delete)'
    - 'writing incremental state (value => 2019-06-01'

- id: 68
  name: 'Run sling hooks & source partitioned (incremental)'
  run: 'sling run -r cmd/sling/tests/replications/r.19.yaml -d --streams ''test1k_postgres_pg_parquet'''
  rows: '>78'
  output_contains:
    - 'skipped hook "start-02"'
    - 'hook (inspect_file_check) failed => check failure'
    - 'writing incremental state (value => 2019-07-01'

- id: 69
  name: Run sling pipeline 01
  run: 'sling run -p cmd/sling/tests/pipelines/p.01.yaml -d'
  output_contains:
    - 'executed step "step-02" (type: replication)'

- id: 70
  name: Run sling chunking
  run: |
    THREADS=5 MODE=full-refresh sling run -r $YAML_FILE -d
    THREADS=6 MODE=truncate     sling run -r $YAML_FILE -d
    THREADS=1 MODE=backfill     sling run -r $YAML_FILE -d
    SLING_STREAM_CNT=18 THREADS=2 MODE=incremental  sling run -r $YAML_FILE -d

    SLING_STREAM_CNT=2 MODE=full-refresh sling run -r $YAML_EXPR_FILE -d
    SLING_STREAM_CNT=2 MODE=truncate     sling run -r $YAML_EXPR_FILE -d
    SLING_STREAM_CNT=2 MODE=incremental  sling run -r $YAML_EXPR_FILE -d
  streams: 33
  env:
    YAML_FILE: cmd/sling/tests/replications/r.20.chunking.yaml
    YAML_EXPR_FILE: cmd/sling/tests/replications/r.20.chunking.expr.yaml
  output_contains:
    - '"update_dt" >= ''2018-11-21'
    - '"id" >= 601 and "id" <= 800'
    - TEST1K_SQLSERVER_PG_003
    - TEST1K_SNOWFLAKE_PG_004
    - ("date" >= '2020-09-26' and "date" <= '2021-04-23') # test1k_duckdb_pg, chunk_count: 4
    - ("id" >= 502 and "id" <= 668)  # test1k_ducklake_pg, chunk_count: 6
    # - ("update_dt" >= '2019-11-16 20:39:39' # test1k_clickhouse_pg, incremental
    - ("id" >= 836  # test1k_ducklake_pg, incremental
    - '"id" <= 1002 /* custom-sql */' # test1k_ducklake_pg_sql, incremental
    - '"update_dt" <= ''2019-11-16 20:39:39'' /* custom-sql */' # test1k_duckdb_pg_sql, incremental
    - ((mod(abs(hashtext(coalesce(first_name, ''))), 2)) = 0) # test1k_mysql_pg, chunk_expr
    - id <= 500 and ("id" >= 1 and "id" <= 500) # test1k_mysql_pg_sql

- id: 71
  name: Run sling pipeline 02
  run: 'sling run -p cmd/sling/tests/pipelines/p.02.yaml'
  output_contains:
    - state.copy_sftp_azure.bytes_written    # ensures a file was copied from SFTP to AZURE
    - state.copy_s3_azure.bytes_written      # ensures a file was copied from S3 to AZURE

- id: 72
  name: Run sling to test column casing
  run: 'sling run -d -r cmd/sling/tests/replications/r.21.yaml'
  streams: 1
  output_contains:
    - test1k_clickhouse_pg_first_name

- id: 73
  name: Run sling mysql bit
  run: 'sling run -d -r cmd/sling/tests/replications/r.22.mysql_bit_type.yaml'
  streams: 1
  output_contains:
    - execution succeeded

- id: 74
  name: Run sling iceberg_r2 insert
  run: |
    sling run -d -r cmd/sling/tests/replications/r.23.iceberg_write.yaml
    sling run --src-conn $TARGET --src-stream public.test1k_mariadb_pg --stdout -l 5
  env:
    TARGET: iceberg_r2
  streams: 2
  output_contains:
    - committed iceberg snapshot
    - count_result => {"count_star":1001}
    - inserted 967 rows
    
    # part of stdout output
    - writing to target stream (stdout)
    - wrote 5 rows

- id: 75
  name: Run sling iceberg_s3 insert
  run: |
    sling run -d -r cmd/sling/tests/replications/r.23.iceberg_write.yaml
    sling run --src-conn $TARGET --src-stream public.test1k_mariadb_pg --stdout -l 5
  env:
    TARGET: iceberg_s3
  streams: 2
  output_contains:
    - committed iceberg snapshot
    - count_result => {"count_star":1001}
    - inserted 967 rows
    
    # part of stdout output
    - writing to target stream (stdout)
    - wrote 5 rows

# - id: 76
#   name: Run sling iceberg_lakekeeper insert
#   run: |
#     sling run -d -r cmd/sling/tests/replications/r.23.iceberg_write.yaml
#     sling run --src-conn $TARGET --src-stream public.test1k_mariadb_pg --stdout -l 5
#   env:
#     TARGET: iceberg_lakekeeper
#   streams: 2
#   output_contains:
#     - committed iceberg snapshot
#     - count_result => {"count_star":1001}
#     - inserted 967 rows
    
#     # part of stdout output
#     - writing to target stream (stdout)
#     - wrote 5 rows

- id: 77
  name: Run sling iceberg_sql insert
  run: |
    sling run -d -r cmd/sling/tests/replications/r.23.iceberg_write.yaml
    sling run --src-conn $TARGET --src-stream public.test1k_mariadb_pg --stdout -l 5
  env:
    TARGET: iceberg_sql
  streams: 2
  output_contains:
    - committed iceberg snapshot
    - inserted 967 rows
    
    # part of stdout output
    - writing to target stream (stdout)
    - wrote 5 rows

- id: 78
  name: Run sling iceberg_glue insert
  run: |
    sling run -d -r cmd/sling/tests/replications/r.23.iceberg_write.yaml
    sling run --src-conn $TARGET --src-stream public.test1k_mariadb_pg --stdout -l 5
  env:
    TARGET: iceberg_glue
  streams: 2
  output_contains:
    - committed iceberg snapshot
    - inserted 967 rows
    
    # part of stdout output
    - writing to target stream (stdout)
    - wrote 5 rows

# - id: 79
#   name: Run sling iceberg_gcp insert
#   run: |
#     sling run -d -r cmd/sling/tests/replications/r.23.iceberg_write.yaml
#     sling run --src-conn $TARGET --src-stream public.test1k_mariadb_pg --stdout -l 5
#   streams: 2
#   env:
#     TARGET: iceberg_gcp
#   output_contains:
#     - committed iceberg snapshot
#     - inserted 967 rows
    
#     # part of stdout output
#     - writing to target stream (stdout)
#     - wrote 5 rows

- id: 80
  name: Prometheus buffer fix test
  run: sling run -d -r cmd/sling/tests/replications/r.25.prometheus_buffer.yaml
  streams: 1
  min_rows: 1
  output_contains:
    - 'using range'
    - "changing column type via transform for 'timestamp': bigint => string"
    - "changing column type via transform for 'value': decimal => string"
    - "'timestamp':'text', 'value':'text'" # the timestamp & value columns were successfully casted to string

- id: 81
  name: Prometheus issue 551 (https://github.com/slingdata-io/sling-cli/issues/551)
  run: sling run -d -r cmd/sling/tests/replications/r.26.prometheus_issue551.yaml
  streams: 1
  output_contains:
    - 'using chunked streaming'

- id: 82
  name: Pipeline copy single file from list result (fix for duplicate filename issue)
  run: sling run -d -p cmd/sling/tests/pipelines/p.03.test_copy_fix_demonstration.yaml
  output_contains:
    - 'copying single file from'
    - 'Copy result:'
    - 'Output files:'
    - 'listing path: /tmp/demo_file_1.csv'
  output_does_not_contain:
    - 'listing path: /tmp/remote'

# - id: 83
#   name: Delta R2 secret creation test (https://github.com/slingdata-io/sling-cli/issues/564)
#   run: sling run -d -r cmd/sling/tests/replications/r.27.delta_r2.yaml
#   err: true  # We expect this to fail because the delta file doesn't exist
#   output_contains:
#     - "delta_scan('r2://"  # Verifies the URL was transformed from s3:// to r2://
#     - 'No files in log segment'
#   # This test validates that the R2 secret is created with ACCOUNT_ID
#   # It will fail at data reading stage, but that proves the secret was created correctly

- id: 84
  name: Oracle NUMBER scale fix (https://github.com/slingdata-io/sling-cli/issues/584)
  run: sling run -r cmd/sling/tests/replications/r.28.oracle_number_scale.yaml
  env:
    SCHEMA: ORACLE
    SOURCE: ORACLE
    TARGET: SNOWFLAKE
  output_contains:
    - 'execution succeeded'

- id: 85
  name: Oracle NUMBER scale fix (https://github.com/slingdata-io/sling-cli/issues/584)
  run: sling run -r cmd/sling/tests/replications/r.28.oracle_number_scale.yaml
  env:
    SCHEMA: ORACLE
    SOURCE: ORACLE
    TARGET: POSTGRES
  output_contains:
    - 'execution succeeded'

- id: 86
  name: MSSQL to parquet overwrite fix (https://github.com/slingdata-io/sling-cli/issues/588)
  run: sling run -r cmd/sling/tests/replications/r.29.mssql_parquet_overwrite.yaml
  output_contains:
    - 'execution succeeded'

- id: 87
  name: MSSQL to Azure Storage multiple files fix (https://github.com/slingdata-io/sling-cli/issues/586)
  run: sling run -r cmd/sling/tests/replications/r.30.mssql_azure_multiple_files.yaml
  output_contains:
    - 'execution succeeded'

- id: 88
  name: Test timestamp to string casting without quotes
  run: sling run -r cmd/sling/tests/replications/r.31.timestamp_string_quotes.yaml --debug
  rows: 3
  output_contains:
    - wrote 3 rows
    - execution succeeded

- id: 89
  name: Test inspect hook for database tables and files
  run: sling run -p cmd/sling/tests/pipelines/p.04.test_inspect_hook.yaml
  output_contains:
    - 'Database table inspection results:'
    - 'S3 file inspection results:'
    - 'Local file inspection results:'
    - 'Local directory inspection results:'
    - 'Path: sling-cli-tests/inspect_test.txt'
    - 'FDQN: "public"."test_inspect_table"'
    - 'Path: /tmp/sling/inspect_test/test_file.json'
    - 'URI: file:///tmp/sling/inspect_test/'
    - 'Is Dir: true'

- id: 90
  name: starrocks empty_as_null
  run: sling run -d -r cmd/sling/tests/replications/r.24.starrocks_empty_as_null.yaml
  streams: 1
  rows: 5
  output_contains:
    - 'store.result'
  output_does_not_contain:
    - 'WARN: Using INSERT mode'

- id: 91
  name: Test encoding source options (latin1, windows1252, utf8_bom) and target encoding
  run: |
    sling run -d -r cmd/sling/tests/replications/r.32.encoding_test.yaml
    sling run -d -r cmd/sling/tests/replications/r.33.encoding_target_test.yaml
  output_contains:
    - 'Latin1 results:'
    - 'José'
    - 'Café manager'
    - 'Windows1252 results:'
    - 'UTF-8 BOM results:'
    - '张三'
    - '김철수'
    - 'wrote 3 rows'
    - 'execution succeeded'

- id: 92
  name: Test encoding garbled output when not specified
  run: sling run -d -r cmd/sling/tests/replications/r.34.encoding_garbled_test.yaml
  output_contains:
    - 'SUCCESS: Correct output contains proper special characters'
    - 'SUCCESS: Garbled output has corrupted special characters as expected'
    - 'execution succeeded'

- id: 93
  name: Test transform functions
  run: sling run -d -r cmd/sling/tests/replications/r.35.transform_functions_test.yaml
  output_contains:
    - 'Transform Functions Test Results'
    - 'execution succeeded'
  output_does_not_contain:
    - "did find find transform with params named: 'now'"

- id: 94
  name: Test mongo column casing selection
  run: sling run -d -r cmd/sling/tests/replications/r.36.mongo_snowflake_select.yaml
  output_contains:
    - '"projection":[{"Key":"id","Value":1},{"Key":"first_name","Value":1},{"Key":"last_name","Value":1}]}'
  output_does_not_contain:
     # before, it would upper-case the columns upon column listing
    - '"projection":[{"Key":"ID","Value":1},{"Key":"FIRST_NAME","Value":1},{"Key":"LAST_NAME","Value":1}]}}'

- id: 95
  name: Test decimal to string casting with proper varchar length (https://github.com/slingdata-io/sling-cli/issues/599)
  run: sling run -d -r cmd/sling/tests/replications/r.37.decimal_to_string_cast.yaml
  output_contains:
    - 'Result from Snowflake:'
    - '"100.000"'
    - '"-999.999"'
    - '"0.00001"'

- id: 96
  name: Test extra files deletion (https://github.com/slingdata-io/sling-cli/issues/614)
  run: sling run -d -p cmd/sling/tests/pipelines/p.05.extra_parquet_files_deletion.yaml
  output_does_not_contain:
    - step (extra_files_check) failed

- id: 97
  name: Test to ensure snowflake compression for upload
  run: sling run -d -r cmd/sling/tests/replications/r.38.snowflake_compression_test.yaml
  output_contains:
    - compression=zstd
    - compression=gzip

- id: 98
  name: Test SLING_LOADED_AT column type when casting all columns to string
  run: |
    sling run -d -r cmd/sling/tests/replications/r.39.sling_loaded_at_string_cast.yaml
    sling run -d -r cmd/sling/tests/replications/r.40.sling_loaded_at_csv_string_cast.yaml
  output_contains:
    - 'SUCCESS: _SLING_LOADED_AT is correctly kept as TIMESTAMP even when all columns are cast to string (Postgres source)'
    - "SUCCESS: _SLING_LOADED_AT is correctly kept as TIMESTAMP even when all columns are cast to string (CSV source)"

# - id: 99
#   name: Test empty dataset exporting with Parquet (https://github.com/slingdata-io/sling-cli/issues/598)
#   run: |
#     sling run -d -r cmd/sling/tests/replications/r.41.redshit_empty_parquet.yaml
#   output_contains:
#     - no data or records found in stream

- id: 100
  name: Test excluding column when exporting to Parquet https://github.com/slingdata-io/sling-cli/issues/607)
  run: |
    sling run -d -r cmd/sling/tests/replications/r.42.mssql_exclude_column_issue607.yaml
  output_contains:
    - 'extra_col column count in parquet file: 0'

- id: 101
  name: Test Azure Table incremental mode to PostgreSQL
  run: |
    MODE=full-refresh sling run -d -r cmd/sling/tests/replications/r.43.azuretable_incremental_postgres.yaml --limit 5
    MODE=incremental sling run -d -r cmd/sling/tests/replications/r.43.azuretable_incremental_postgres.yaml
  streams: 1
  output_contains:
    - 'Loaded 5 rows into public.test1k_azuretable_pg'
    - 'inserted 213 rows'
    - 'rows into public.test1k_azuretable_pg'
    - 'id, first_name, email, create_dt, Timestamp'

- id: 102
  name: Test staged transforms with MySQL source and PostgreSQL target
  run: sling run -d -r cmd/sling/tests/replications/r.44.staged_transforms_test.yaml
  streams: 3
  rows: 15  # 3 * 5 = 15
  output_contains:
    - 'Staged Transforms Test Results'
    - 'execution succeeded'

- id: 103
  name: Test Exasol large CSV file ingestion (100K rows)
  run: sling run -d -r cmd/sling/tests/replications/r.45.exasol_large_csv.yaml
  streams: 1
  rows: 100000
  output_contains:
    - 'inserted 100000 rows'
    - 'Row count: 100000'
    - 'execution succeeded'

- id: 104
  name: Test PostgreSQL & BigQury TIME datatype preservation
  run: |
    sling run -d -r cmd/sling/tests/replications/r.46.postgres_time_datatype.yaml
    sling run -d -r cmd/sling/tests/replications/r.48.bigquery_time_datatype.yaml
  output_contains:
    - '| time without time zone |'
    - '| time with time zone    |'
    - | # pg output
      +----+----------+--------------------+----------------+
      | ID | TIME_COL | TIME_PRECISION_COL | TIME_TZ_COL    |
      +----+----------+--------------------+----------------+
      | 1  | 08:30:45 | 08:30:45.123       | 08:30:45+02:00 |
      | 2  | 12:15:30 | 12:15:30.456       | 12:15:30-05:00 |
      | 3  | 23:59:59 | 23:59:59.789       | 23:59:59+00:00 |
      +----+----------+--------------------+----------------+
    - | # bq output
      +----+----------+--------------------+----------------+
      | ID | TIME_COL | TIME_PRECISION_COL | TIME_TZ_COL    |
      +----+----------+--------------------+----------------+
      | 1  | 08:30:45 | 08:30:45.123       | 08:31:45+02:00 |
      | 2  | 12:15:30 | 12:15:30.456       | 12:11:30-05:00 |
      | 3  | 23:59:59 | 23:59:59.789       | 23:51:59+00:00 |
      +----+----------+--------------------+----------------+

- id: 105
  name: Test MSSQL uniqueidentifier datatype preservation
  run: |
    sling run -d -r cmd/sling/tests/replications/r.47.mssql_uniqueidentifier.yaml
    sling run -d -r cmd/sling/tests/replications/r.47.mssql_uniqueidentifier.postgres.yaml
  output_contains:
    - "guid_col should be 'uniqueidentifier' type in target"
    - "GUID value for record 1 should match"
    - "GUID value for record 5 should match"
    - "execution succeeded"
    - "DROP TABLE public.unique_identifier_test2"

- id: 106
  name: Test MSSQL to BigQuery BIGNUMERIC conversion for high precision/scale decimals
  run: sling run -d -r cmd/sling/tests/replications/r.49.mssql_bigquery_bignumeric.yaml
  streams: 1
  rows: 3
  output_contains:
    - "Successfully retrieved column information from BigQuery"
    - "The BIGNUMERIC conversion logic is working correctly!"
    - "All 3 test records should be transferred"
    - "Data integrity verified - high precision decimal values transferred successfully"
    - "execution succeeded"

- id: 107
  name: Test CSV to BigQuery BIGNUMERIC conversion for high scale decimal values
  run: sling run -d -r cmd/sling/tests/replications/r.50.csv_bigquery_bignumeric.yaml
  streams: 1
  rows: 10
  output_contains:
    - "high_scale_1 correctly converted to BIGNUMERIC"
    - "high_scale_2 correctly converted to BIGNUMERIC"
    - "edge_scale_10 (scale=10 > 9) correctly converted to BIGNUMERIC"
    - "very_high_scale correctly converted to BIGNUMERIC"
    - "normal_scale correctly remains as NUMERIC"
    - "small_decimal correctly remains as NUMERIC"
    - "All 10 CSV rows loaded successfully"
    - "High scale decimal value 1 preserved accurately"
    - "Negative high scale decimal preserved accurately"
    - "Very small high scale decimal preserved accurately"
    - "Edge scale=10 BIGNUMERIC value preserved accurately"
    - "Normal scale NUMERIC value preserved accurately"
    - "Small decimal NUMERIC value preserved accurately"
    - "Edge scale=9 NUMERIC value preserved accurately"
    - "Very high scale BIGNUMERIC value preserved accurately"
    - "CSV to BigQuery BIGNUMERIC conversion test completed successfully!"
    - "execution succeeded"

- id: 108
  name: Run XML to PostgreSQL import test
  run: 'sling run -r cmd/sling/tests/replications/r.51.xml_postgres_import.yaml'
  streams: 1
  rows: 5
  output_contains:
    - 'execution succeeded'
    - '✓ All 5 XML records imported successfully'
    - '✓ Name field imported correctly'
    - '✓ Email field imported correctly'
    - '✓ Age field imported correctly'
    - '✓ Salary field imported correctly'
    - '✓ XML to PostgreSQL import test completed successfully!'

- id: 109
  name: Test API -- Sling Platform
  run: sling run -r cmd/sling/tests/replications/apis/r.sling_platform.yaml -d
  streams: 3
  output_contains:
    - 'execution succeeded'
    
    # test iterations
    - '/connection/list?test-value=1'
    - '/connection/list?test-value=2'
    - '/connection/list?test-value=3'
    - '/connection/list?test-value=4'
    - '/execution/list?filters=%7B%22period'

- id: 110
  name: Test API -- Sling Stripe
  run: sling run -r cmd/sling/tests/replications/apis/r.stripe.yaml
  output_contains:
    - 'execution succeeded'

- id: 111
  name: Run AWS S3 file splitting test
  run: 'sling run -r cmd/sling/tests/replications/r.52.parquet_file_splitting.yaml'
  env:
    MY_TARGET: aws_s3
  streams: 1
  rows: 1000
  output_contains:
    - 'execution succeeded'

- id: 112
  name: Run AZURE_STORAGE file splitting test
  run: 'sling run -r cmd/sling/tests/replications/r.52.parquet_file_splitting.yaml'
  env:
    MY_TARGET: AZURE_STORAGE
  streams: 1
  rows: 1000
  output_contains:
    - 'execution succeeded'

- id: 113
  name: Run GOOGLE_STORAGE file splitting test
  run: 'sling run -r cmd/sling/tests/replications/r.52.parquet_file_splitting.yaml'
  env:
    MY_TARGET: GOOGLE_STORAGE
  streams: 1
  rows: 1000
  output_contains:
    - 'execution succeeded'

- id: 114
  name: Run local file splitting test
  run: 'sling run -r cmd/sling/tests/replications/r.52.parquet_file_splitting.yaml'
  env:
    MY_TARGET: local
  streams: 1
  rows: 1000
  output_contains:
    - 'execution succeeded'

# Test Prometheus with no data (should not error)
- id: 115
  run: 'sling run -r cmd/sling/tests/replications/r.53.prometheus_incremental.yaml'
  env:
    SLING_ALLOW_EMPTY: 'TRUE'
  conns:
    - prometheus
    - postgres
  streams: 1
  rows: 0
  output_contains:
    - 'execution succeeded'

# Test MongoDB ObjectID filtering
- id: 116
  name: MongoDB filter test
  run: |
    sling run -r cmd/sling/tests/replications/r.54.mongo_filter.yaml
    sling run -r cmd/sling/tests/replications/r.54.mongo_objectid_filter.yaml

- id: 117
  name: JSON to Snowflake nested import test
  run: 'sling run -d -r cmd/sling/tests/replications/r.55.json_snowflake_nested.yaml'

# Test SQL Server money type to StarRocks
- id: 118
  name: SQL Server money type to StarRocks
  run: 'sling run -d -r cmd/sling/tests/replications/r.56.mssql_starrocks_money.yaml'
  conns:
    - mssql
    - starrocks
  output_contains:
    - 'execution succeeded'
    - 'Row-by-row comparison'

- id: 119
  name: Run replication test for RunState properties
  run: 'sling run -r cmd/sling/tests/replications/r.57.run_state_test.yaml'
  streams: 2
  rows: 10
  output_contains:
    - run.total_bytes = 948
    - run.total_rows = 10
    - run.status = success
    - '   id = public_run_state_test'

- id: 120
  name: Test PostgreSQL mixed encoding character handling
  run: 'sling run -d -r cmd/sling/tests/replications/r.58.postgres_invalid_utf8.yaml'
  rows: 10
  output_contains:
    - 'execution succeeded'

- id: 121
  name: Test MSSQL to PostgreSQL encoding handling
  run: 'sling run -d -r cmd/sling/tests/replications/r.59.mssql_postgres_encoding.yaml'
  rows: 10
  output_contains:
    - 'execution succeeded'

- id: 122
  name: Test MSSQL to PostgreSQL collations handling
  run: 'sling run -d -r cmd/sling/tests/replications/r.60.mssql_postgres_collations.yaml'
  rows: 5
  output_contains:
    - 'execution succeeded'

- id: 123
  name: Test MSSQL identity columns with incremental mode (Issue #657)
  run: 'sling run -d -r cmd/sling/tests/replications/r.61.mssql_identity_incremental.yaml'
  output_contains:
    - 'execution succeeded'

# with add_new_columns=false, we should WARN instead of erroring
- id: 124
  name: Test MSSQL to PostgreSQL with add_new_columns=false
  run: 'sling run -d -r cmd/sling/tests/replications/r.62.mssql_postgres_add_columns_false.yaml'
  output_contains:
    - source field 'age' is missing in target table

# Test PostgreSQL to BigQuery with all data types using CSV and Parquet format
- id: 125
  name: Test PostgreSQL to BigQuery with all data types
  run: 'sling run -d -r cmd/sling/tests/replications/r.63.postgres_bigquery_all_types.yaml'
