source: local
target: exasol

defaults:
  mode: full-refresh

hooks:
  start:
    # Create a large CSV file with 100,000 rows
    - type: command
      command: |
        python3 -c "
        import csv
        with open('/tmp/large_test_file.csv', 'w', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(['id', 'name', 'email', 'status', 'timestamp'])
            for i in range(1, 100001):
                day = ((i % 28) + 1)
                hour = (i % 24)
                minute = (i % 60)
                sec = (i % 60)
                timestamp = '2024-01-%02d %02d:%02d:%02d' % (day, hour, minute, sec)
                writer.writerow([i, 'User_%d' % i, 'user_%d@example.com' % i, 'active', timestamp])
        "

  end:
    # Verify the row count
    - type: query
      connection: '{target.name}'
      query: select count(*) as row_count from PUBLIC.LARGE_CSV_TEST
      into: count_result
    
    - type: log
      message: |
        Row count result: {store.count_result}
        Row count: {store.count_result[0].row_count}
    
    - type: check
      check: int_parse(store.count_result[0].row_count) == 100000
    
    # Clean up the table
    - type: query
      connection: '{target.name}'
      query: drop table if exists PUBLIC.LARGE_CSV_TEST
    
    # Clean up the CSV file
    - type: command
      command: rm -f /tmp/large_test_file.csv

streams:
  file:///tmp/large_test_file.csv:
    object: public.large_csv_test
    target_options:
      file_max_rows: 40000